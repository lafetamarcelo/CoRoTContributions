{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data\n",
    "\n",
    "To go trough with this example, one might have or a \n",
    "\n",
    "- *filtered.pkl* - pickle format file\n",
    "- *filtered.mat* - matlab format file\n",
    "\n",
    "as generated in the previous step of the pipeline. Note that, we are now ready to generate features for the machine learning algorithms. The biggest challenge is to get static features out of the time series (wich is a dynamic data). Here we will approach three main static feature generation paths:\n",
    "\n",
    "- Frequency analysis with periodograms\n",
    "- Naive Bayes likelihood parameters\n",
    "- Markov transition probability matrix\n",
    "\n",
    "All of these three features will be further used as static data for the machine learning algorithms to learn how to classify the light curves as each class. So first, it is necessary to read the preprocessed/filtered data from the last pipeline step:\n",
    "\n",
    "> *Note that we must have the data preprocessed and labeled as exo-planets and not exo-planets. If the user does not have this file already, it just need to run follow trough the last pipeline step procedures, or one can download one version of the preprocessed data from:*\n",
    "- [Google drive access](https://drive.google.com/drive/folders/19kALbQ5m1ppXxGTVMBaWA4KdIE9pmWfM?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "file_path = './filtered.pkl'\n",
    "with open(file_path, 'rb') as f:\n",
    "    curves = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After, lets import the utils package with the support algorithms..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.0.1.min.js\": \"JpP8FXbgAZLkfur7LiK3j9AGBhHNIvF742meBJrjO2ShJDhCG2I1uVvW+0DUtrmc\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.0.1.min.js\": \"xZlADit0Q04ISQEdKg2k3L4W9AwQBAuDs9nJL9fM/WwzL1tEU9VPNezOFX0nLEAz\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.0.1.min.js\": \"4BuPRZkdMKSnj3zoxiNrQ86XgNw0rYmBOxe7nshquXwwcauupgBF2DHLVG1WuZlV\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.0.1.min.js\": \"Dv1SQ87hmDqK6S5OhBf0bCuwAEvL5QYL0PuR/F1SPVhCS/r/abjkbpKDYL2zeM19\"};\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.0.1.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.0.1.min.js\": \"JpP8FXbgAZLkfur7LiK3j9AGBhHNIvF742meBJrjO2ShJDhCG2I1uVvW+0DUtrmc\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.0.1.min.js\": \"xZlADit0Q04ISQEdKg2k3L4W9AwQBAuDs9nJL9fM/WwzL1tEU9VPNezOFX0nLEAz\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.0.1.min.js\": \"4BuPRZkdMKSnj3zoxiNrQ86XgNw0rYmBOxe7nshquXwwcauupgBF2DHLVG1WuZlV\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.0.1.min.js\": \"Dv1SQ87hmDqK6S5OhBf0bCuwAEvL5QYL0PuR/F1SPVhCS/r/abjkbpKDYL2zeM19\"};\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.0.1.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just plot an example time series to get a notion on the data obtained from the pickle (*.pkl*) file generated from the previous study script, and for that the `utils` library will be imported to take advantage of its `visual` functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "index = 8\n",
    "\n",
    "time_in_days = [curves['i'][index] \n",
    "                + timedelta(minutes=time) for time in curves['t'][index]]\n",
    "\n",
    "x_data = [time_in_days, time_in_days]\n",
    "y_data = [curves['r'][index], curves['y'][index]]\n",
    "legends= ['Raw Light Curve', 'Filtered Light Curve']\n",
    "colors = [2, 5]\n",
    "\n",
    "p = visual.multline_plot(x_data, y_data,\n",
    "                         legend_label=legends, \n",
    "                         title='Light Curve Example',\n",
    "                         color_index=colors,\n",
    "                         y_axis={'label': 'Intensity'},\n",
    "                         x_axis={'label': 'Date (m/dd)',\n",
    "                                 'type': 'datetime'})\n",
    "#visual.show_plot(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for ilustration... lets see how much samples we have in each curve. This gives the notion on how much data there is in each curve. From that it is possible to realise how much time the next algorithms will take to generate the necessary information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [len(curve) for curve in curves['y']]\n",
    "x_values = [k+1 for k in range(len(sizes))]\n",
    "sizes.sort(reverse=True)\n",
    "\n",
    "p = visual.line_plot(x_values, sizes,\n",
    "                     legend_label='Dimensions', \n",
    "                     title='Curves dimensions',\n",
    "                     color_index=2,\n",
    "                     y_axis={'label': 'Sizes'},\n",
    "                     x_axis={'label': 'Curve index'})\n",
    "#visual.show_plot(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute some important constants that will be used during the analysis, such as mean sample time and sample frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The series have a time sample of 8.56 minutes, consequently a sample frequency of 0.001946 Hz\n"
     ]
    }
   ],
   "source": [
    "sample_time = curves['t'][0][1] - curves['t'][0][0]   # minutes\n",
    "sample_freq = 1 / (60 * sample_time)                  # hertz\n",
    "\n",
    "print(\"The series have a time sample of {} minutes, consequently a sample frequency of {} Hz\".format(round(sample_time,2), round(sample_freq,6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature: Frequency response\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "One must know that theorically the most characteristic representation of a particular signal, would be its Fourier spectrum. If the real life signals were actually simple, it would be pretty simple to characterize those signals by their frequency spectrum. But unfortunately the real world signals are actully filled with noise from several natures.\n",
    "\n",
    "So, if you want to get the most informative static information of a dynamic signal, it would be the power spectrum of this signal in the frequency domain without the undesired noise. In this chapter, it is developed an algorithm that attempt to find this representation for each light curve and adapt this information to be further used as features for machine learning algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "Select one curve to create the frequency analysis, let's say the curve indexed as `index = 5`. We use that curve as an example to create the routine to create time-series power spectrum feature for the machine learning algorithms. Later, a generation algorithm will be presented, which will properly reproduce this process for all handle light curves. For that matter, we first present the curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 5\n",
    "\n",
    "time_in_days = [curves['i'][index] \n",
    "                + timedelta(minutes=time) for time in curves['t'][index]]\n",
    "\n",
    "x_data = [time_in_days, time_in_days]\n",
    "y_data = [curves['r'][index], curves['y'][index]]\n",
    "legends= ['Raw Light Curve', 'Filtered Light Curve']\n",
    "colors = [2, 5]\n",
    "\n",
    "p = visual.multline_plot(x_data, y_data,\n",
    "                         legend_label=legends, \n",
    "                         title='Light Curve Example',\n",
    "                         color_index=colors,\n",
    "                         y_axis={'label': 'Intensity'},\n",
    "                         x_axis={'label': 'Date (m/dd)',\n",
    "                                 'type': 'datetime'})\n",
    "#visual.show_plot(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectrum generation\n",
    "\n",
    "\n",
    "Then we can use the `signal` library from `scipy` to create the Periodogram or also called as the Spectrogram of this time series. We will create both the frequency information for the filtered signal, and the original data saved from the last analysis, just to highlight the information removed with the filtering technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal as ssg\n",
    "\n",
    "freq, spectra = ssg.periodogram(curves['r'][index], \n",
    "                                fs=sample_freq, scaling='density')\n",
    "ffreq, fspectra = ssg.periodogram(curves['y'][index], \n",
    "                                  fs=sample_freq, scaling='density')\n",
    "efreq, espectra = ssg.periodogram(curves['y'][index]-curves['r'][index], \n",
    "                                  fs=sample_freq, scaling='density')\n",
    "\n",
    "x_data = [freq, ffreq]\n",
    "y_data = [spectra, fspectra]\n",
    "legends= ['Raw LC Spectrum', 'Filtered LC Spectrum']\n",
    "colors = [2, 5]\n",
    "\n",
    "p = visual.multline_plot(x_data, y_data,\n",
    "                         legend_label=legends, \n",
    "                         title='Light Curve Frequency Spectrum',\n",
    "                         color_index=colors,\n",
    "                         y_axis={'label': 'Magnitude'},\n",
    "                         x_axis={'label': 'Frequency (Hz)',\n",
    "                                 'type': 'log'})\n",
    "p1 = visual.line_plot(efreq, espectra,\n",
    "                     legend_label='Difference spectra', \n",
    "                     title='Spectrum of the filtered out noise',\n",
    "                     color_index=3,\n",
    "                     y_axis={'label': 'Magnitude'},\n",
    "                     x_axis={'label': 'Frequency (Hz)',\n",
    "                             'type': 'log'})\n",
    "\n",
    "#visual.show_plot(p, p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *From here, one might see that the filtering technique applied on the last analysis, only remove high frequency compenents from the data. We don't know yep if the removed information is important or not for future analysis with machine learning, therefore, both frequency responses will be saved as feature for further analysis.*\n",
    "\n",
    "\n",
    "\n",
    "## Detrended spectrum\n",
    "\n",
    "---\n",
    "\n",
    "But before generating the proposed feature for the machine learning, we must first present another possible process of the signal that might be relevant for further analysis. The DC level of the signal, usually represents the $0$ Hz component of the frequency spectrum, and does not provide any dynamic meaningful information of the data. Therefore, it is common sence to first remove the so called trend that composes the DC level of the signal. This process is also called detrending of the signal. And it is pretty simple to be applied using the `signal` library from `scipy`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal as ssg\n",
    "\n",
    "detrended_data = ssg.detrend(curves['y'][index], type='linear')\n",
    "\n",
    "time_in_days = [curves['i'][index] \n",
    "                + timedelta(minutes=time) for time in curves['t'][index]]\n",
    "\n",
    "p = visual.line_plot(time_in_days, curves['y'][index],\n",
    "                     legend_label='Raw Light Curve', \n",
    "                     title='Light Curve Raw',\n",
    "                     color_index=2,\n",
    "                     y_axis={'label': 'Intensity'},\n",
    "                     x_axis={'label': 'Time (dd/mm/yy)'})\n",
    "p1 = visual.line_plot(time_in_days, detrended_data,\n",
    "                     legend_label='Detrended Light Curve', \n",
    "                     title='Light Curve Detrended',\n",
    "                     color_index=4,\n",
    "                     y_axis={'label': 'Intensity'},\n",
    "                     x_axis={'label': 'Time (dd/mm/yy)'})\n",
    "#visual.show_plot(p, p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The influence on the periodogram can be shown by just generating once more the periodogram of the detrended signal and the previous one, with DC level influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq, spectra = ssg.periodogram(curves['y'][index], fs=sample_freq, scaling='spectrum')\n",
    "ffreq, fspectra = ssg.periodogram(detrended_data, fs=sample_freq, scaling='spectrum')\n",
    "efreq, espectra = ssg.periodogram(detrended_data-curves['y'][index], fs=sample_freq, scaling='spectrum')\n",
    "\n",
    "x_data = [freq, ffreq]\n",
    "y_data = [spectra, fspectra]\n",
    "legends= ['Raw LC Spectrum', 'Detrended LC Spectrum']\n",
    "colors = [2, 5]\n",
    "\n",
    "p = visual.multline_plot(x_data, y_data,\n",
    "                         legend_label=legends, \n",
    "                         title='Light Curve Spectrum',\n",
    "                         color_index=[2, 5],\n",
    "                         y_axis={'label': 'Magnitude'},\n",
    "                         x_axis={'label': 'Frequency (Hz)',\n",
    "                                 'type': 'log'})\n",
    "p1 = visual.line_plot(efreq, espectra,\n",
    "                     legend_label='LC Difference Spectrum',\n",
    "                     title='Difference Spectrum',\n",
    "                     color_index=5,\n",
    "                     y_axis={'label': 'Magnitude'},\n",
    "                     x_axis={'label': 'Frequency (Hz)', \n",
    "                             'type': 'log'})\n",
    "#visual.show_plot(p, p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting enough, this is the spectrum of the time series analysed (`index=5`). Of course we will detect several highly evidenced frequency components, because the time series is clearly periodic. That is the most informative data that we could get from this time series using signal and dynamic systems theory. \n",
    "\n",
    "> *One might see that the information removed by using the detrend technique only removes low frequency information in a smooth way. It is necessary to be carreful when using filtering tehcniques, so that no nonlinear and aggressive techniques are applied to the data. Those aggressive techniques are usually not practical and might result on unreal phenomenon on the data.*\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Resample spectrum\n",
    "\n",
    "One might notice that the spectrogram will not have the same resolution for each feature... This is the biggest problem on computing the time series power spectrum: the dimension inconsitency produced by the computation techniques to craft the power spectrum. Usually those algorithms rely on time series windows (remember the convolution process) to estimate the parameters of the Fourier representation of the series. Each time series has a particular number of samples and windows sizes with their sweep space, and those controls the build power spectrum resolution. Because of this, each light curve will have a particular power spectrum resolution.\n",
    "\n",
    "It is therefore, necessary to create an algorithm able to reshape or resample those power spectrum to an unique resolution. This is necessary because most machine learning algorithms only deal with constant feature dimensions. To achieve this goal, it is introduced the `resample_freq_data` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal as ssg\n",
    "from PyAstronomy import pyasl\n",
    "\n",
    "def resample_freq_data(data=None, \n",
    "                       upper_sample=True,\n",
    "                       window=13,\n",
    "                       algorithm='hamming'):\n",
    "    output_data = list()\n",
    "    # Get the maximun and minimun frequencies, and \n",
    "    # the resolution of each feature\n",
    "    fmax, fmin, size = [], [], []\n",
    "    for feat in data['freq']:\n",
    "        fmax.append(feat.max())\n",
    "        fmin.append(feat.min())\n",
    "        size.append(len(feat))\n",
    "    # Compute the high and low resolution step for\n",
    "    # the resampling\n",
    "    high_step = (max(fmax) - min(fmin)) / max(size) # high resolution\n",
    "    low_step = (max(fmax) - min(fmin)) / min(size)  # low resolution\n",
    "    # Define the cutoff frequency to limit the \n",
    "    # spectrum data set, adn compute the new\n",
    "    # signal resolution based on the steps\n",
    "    up_cut_freq = min(fmax)\n",
    "    down_cut_frea = max(fmin)\n",
    "    high_resolution = up_cut_freq / high_step\n",
    "    low_resolution = up_cut_freq / low_step\n",
    "    # Compute the resample of each feature\n",
    "    compact = zip(data['spec'], data['freq'])\n",
    "    for spec, freq in compact:\n",
    "        # Find the closest index to the cut \n",
    "        # off freq, to remove the information \n",
    "        uspec, ufreq = spec, freq\n",
    "        if max(freq) > up_cut_freq:\n",
    "            freq_diff = [abs(f - up_cut_freq) for f in freq]\n",
    "            cut_index = freq_diff.index(min(freq_diff))\n",
    "            uspec = spec[:cut_index+1]\n",
    "        # Compute the resampled signal for\n",
    "        # high or low resolution\n",
    "        if upper_sample:\n",
    "            sig_size = round(high_resolution)\n",
    "        else:\n",
    "            sig_size = round(low_resolution)\n",
    "        # Resample the spectrum\n",
    "        uspec = ssg.resample(uspec, int(sig_size))\n",
    "        # Smooth the resampled spectrum\n",
    "        uspec = pyasl.smooth(uspec, window, algorithm)\n",
    "        # Save the info into the output signal\n",
    "        output_data.append( uspec[:-10] )\n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation algorithm\n",
    "\n",
    "Lets run all the preprocessing for all the light curve time series, and build the feature variable to be saved for the machine learning step of the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "detrend = True\n",
    "filtered = True\n",
    "\n",
    "# Detrend and filter all light curve\n",
    "# time series data\n",
    "size = len(curves['r'])\n",
    "aux_data = {\n",
    "    'freq': [], \n",
    "    'spec': []\n",
    "}\n",
    "for item in range(size):\n",
    "    # If we want a filtered data\n",
    "    if filtered:\n",
    "        data = curves['y'][item]\n",
    "    else:\n",
    "        data = curves['r'][item]\n",
    "    # If we want a detrended data\n",
    "    if detrend:\n",
    "        data = ssg.detrend(data, type='linear')\n",
    "    # Create the periodogram\n",
    "    freq, spec = ssg.periodogram(data, fs=sample_freq, scaling='spectrum')\n",
    "    # Save on the current variable\n",
    "    aux_data['freq'].append( freq )\n",
    "    aux_data['spec'].append( spec ) \n",
    "                                       \n",
    "# Build the machine learning data\n",
    "# structure to be saved on pickle file                                       \n",
    "ml_data = {\n",
    "    'features': {\n",
    "        'spec': resample_freq_data(aux_data,window=23)\n",
    "    },\n",
    "    'labels': curves['lab']\n",
    "}\n",
    "                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to guarantee the quality of the resampled variables, lets just plot the resampled one and the original spectrums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "index_plot = 15\n",
    "\n",
    "# Resample the frequency data\n",
    "resolution = len(ml_data['features']['spec'][index_plot])\n",
    "init = aux_data['freq'][index_plot][0]\n",
    "final = aux_data['freq'][index_plot][-1]\n",
    "rfreq = np.linspace(init, final, resolution)\n",
    "\n",
    "# Create the plot data\n",
    "x_data = [aux_data['freq'][index_plot], rfreq]\n",
    "y_data = [aux_data['spec'][index_plot], ml_data['features']['spec'][index_plot]]\n",
    "legends, colors, lw = ['Original', 'Resampled'], [2, 8], [3, 2]\n",
    "\n",
    "p = visual.multline_plot(x_data, y_data, \n",
    "                     legend_label=legends,\n",
    "                     title='Spectrum validation',\n",
    "                     color_index=colors,\n",
    "                     line_width=lw,\n",
    "                     y_axis={'label': 'Magnitude'},\n",
    "                     x_axis={'label': 'Frequency (Hz)',\n",
    "                             'type': 'log'})\n",
    "\n",
    "#visual.show_plot(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save feature\n",
    "\n",
    "Here we will modify the feature in a way to be ready for machine learning algorithm take this information as features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = './features/freq_data/freq_data.pkl'\n",
    "\n",
    "output = open(file_name, 'wb')\n",
    "pickle.dump(ml_data, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature: Naive Bayes likelihood\n",
    "\n",
    "--- \n",
    "\n",
    "\n",
    "To create the Naive Bayes marginal likelihood approach, we just will try to use the bayesian theory for optimal filtering. The main structure here used will be the Bayes Ridge Regression, from the `sklearn` library. The main idea is to create a regression based model for each curve, and use the estimated parameters of each model as feature for machine learning classification. The regression model can be represented as\n",
    "\n",
    "$$ p(y |X,\\omega, \\alpha) = \\mathcal{N}(y|X\\omega,\\alpha)$$\n",
    "\n",
    "In the ridge regression, is assumed the prior value for the cofficient $\\omega$ to be given by a spherical Gaussian, leading the regression problem to be mapped as\n",
    "\n",
    "$$ p(\\omega |\\lambda) = \\mathcal{N}(\\omega|0,\\lambda^{-1}I_p)$$\n",
    "\n",
    "The model estimation is just a matter of finding the set of $\\omega$, that minimizes the $\\mathcal{l}_2$ norm for a provided user defined parameter $\\lambda$. The $\\omega$ values are the most characteristic information of each light curve. Thence, here we use those values as features to train the classifier to cluster the light curve classes.\n",
    "\n",
    "There are some discussions that one might include for this approach. One might ask: \n",
    "\n",
    "- What question should the bayes alorithm answer? \n",
    "- It will be just a regression model pro predict the next step? \n",
    "- It will be a regression model to predict model tendencies? \n",
    "\n",
    "The best answer for that will appear when one uses the features provided from each path taken to try to classify the model. We cannot say before hand what feature will be best or not... Therefore, we must create some fetaure and then use their information to try to cluster the curve data.\n",
    "\n",
    "## Regression model\n",
    "\n",
    "In both approaches is necessary to create a regression model, with chosen order $n_x$. To do that, one can use a function such as this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_regressor(data=None, order=6, norm=True):\n",
    "    nx = order\n",
    "    outputs = []\n",
    "    regressors = []\n",
    "    for curve in data:\n",
    "        phi, y = [], []\n",
    "        size = len(curve)\n",
    "        # Normalize the curve data\n",
    "        serie = curve\n",
    "        if norm:\n",
    "            serie = (curve - min(curve)) / (max(curve) - min(curve))\n",
    "        # Build the regressor model\n",
    "        for k in range(size-nx):\n",
    "            phi.append(serie[k:k+nx])\n",
    "            y.append(serie[k+nx])\n",
    "        # Save the build regressors\n",
    "        regressors.append(phi)\n",
    "        outputs.append(y)\n",
    "    return regressors, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step parameters\n",
    "\n",
    "Here we will create the regression problem for each light curve, and estimate the respective rigde bayes parameters. For that the `linear_model` library from `sklearn` will be used, specific the `BayesianRidge` object. To both create the regressor, for each light curve and then train the model, one must do the following\n",
    "\n",
    "> *Usually it is interesting to normalize the data before fitting a regression model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "bayes_data = {\n",
    "    'features': {\n",
    "        'params': []\n",
    "    },\n",
    "    'labels': curves['lab']\n",
    "}\n",
    "\n",
    "# Build the regression model\n",
    "regr, out = build_regressor(curves['y'], order=20, norm=False)\n",
    "\n",
    "# Estimate the bayes regression model\n",
    "# for each set of regressor and outputs\n",
    "for phi, y in zip(regr, out):\n",
    "    # Create the model\n",
    "    clf = linear_model.BayesianRidge()\n",
    "    # Estimate the model\n",
    "    clf.fit( phi, y )\n",
    "    # Save the parameters\n",
    "    bayes_data['features']['params'].append( clf.coef_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save feature\n",
    "\n",
    "Here we just save the feature variable in a particular pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = './features/bayes_data/nx_6/bayes_data.pkl'\n",
    "\n",
    "output = open(file_name, 'wb')\n",
    "pickle.dump(bayes_data, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature: Markov Hidden Models\n",
    "\n",
    "--- \n",
    "\n",
    "\n",
    "Here we will develop a time series prediction algorithm using the so called Hidden Markov Models. They are not much more than a state space model without the input signal... a model that freely vary provided an initial condition. The model can be simply mapped as\n",
    "\n",
    "$$x(k+1) = A x(k)$$\n",
    "\n",
    "where our job is to determine the parameter matrix $A$. Note that $x(k)$ has dimension $n_x$ wich is the model complexity, and user provided.\n",
    "\n",
    "> *Here we actully will use a the library called* [`hmmlearn`](https://hmmlearn.readthedocs.io/en/latest/tutorial.html) *to estimate the model.*\n",
    "\n",
    "One might wonder the reason to estimate this prediction model. The idea is simple, the matrix $A$ will caracterize the  main behavior of the dynamic system, and then it is possible this summarized information of the time series (the $A$ matrix) as feature for the classification machine learning algorithm further used. \n",
    "\n",
    "## Preprocessing data\n",
    "\n",
    "But for that to work, we need to maintain the parameters of each estimated $A$ matrix as close as possible to each other. This means that we first need to preprocess the data in a manner to maintain the same signal power within each curve. This can be done by detrending the time series, and then normalizing their values.\n",
    "\n",
    "To detrend the data we will use the same library previouly used above from `scipy.signal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal as ssg\n",
    "\n",
    "# Create the feature data to be\n",
    "# further saved for machine learning\n",
    "hmm_data = {\n",
    "    'y': [],\n",
    "    't': curves['t'],\n",
    "    'labels': curves['lab'],\n",
    "    'features': {\n",
    "        'prob_matrix': []\n",
    "    }\n",
    "}\n",
    "\n",
    "# Flags for the pipeline\n",
    "norm = False\n",
    "\n",
    "# Pre processing pipe line\n",
    "for curve in curves['y']:\n",
    "    # Detrend time series\n",
    "    series = ssg.detrend(curve, type='linear')\n",
    "    # Normalize (0, 1) time series\n",
    "    if norm:\n",
    "        mins, maxs = min(series), max(series)\n",
    "        series = (series - mins) / (maxs - mins)\n",
    "    # Add time series to processing data\n",
    "    hmm_data['y'].append( series )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate HMM\n",
    "\n",
    "Now we can just use the algorithm created to determine the Hidden Markov Model for each curve time series. For each curve, we can fetch the parameter called `transmat_` which is actually the transition probability matrix of the state space model, here known as $A$. From that we will have a $A$ matrix for each curve... with model complexity equal to $n_x=$ `n_components`, therefore making $A \\in \\Re_{(n_x, n_x)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# Model parameters\n",
    "cfgs = {\n",
    "    'n_components': 8,\n",
    "    'covariance_type': 'full',\n",
    "    'n_iter': 100\n",
    "}\n",
    "\n",
    "# Compute each probability matrix\n",
    "for curve_data in hmm_data['y']:\n",
    "    # Create the hmm model\n",
    "    remodel = hmm.GaussianHMM(**cfgs)\n",
    "    # Fit the hmm model to data\n",
    "    remodel.fit(np.array(\n",
    "        curve_data).reshape(len(curve_data),1))\n",
    "    # Recover the probability matrix\n",
    "    hmm_data['features']['prob_matrix'].append( remodel.transmat_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save feature\n",
    "\n",
    "Here we just save the feature variable in a particular pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = './features/hmm_data/nx_8/hmm_data.pkl'\n",
    "\n",
    "output = open(file_name, 'wb')\n",
    "pickle.dump(hmm_data, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download features\n",
    "\n",
    "If the user does not want to run this pipeline step algorithm (since it takes time) one can just try to download the features, in several different setups from the Google Drive:\n",
    "\n",
    "- [Google drive access](https://drive.google.com/drive/folders/19kALbQ5m1ppXxGTVMBaWA4KdIE9pmWfM?usp=sharing)\n",
    "\n",
    "In this drive the user will find a repository called `features`, that will have this structure:\n",
    "\n",
    "```Json\n",
    "./features\n",
    "   │\n",
    "   ├── bayes_data\n",
    "   │     ├── nx_4\n",
    "   │     │    ├── bayes_data.pkl\n",
    "   │     │    └── norm_bayes_data.pkl\n",
    "   │     ├── nx_6\n",
    "   │     │    ├── bayes_data.pkl\n",
    "   │     │    └── norm_bayes_data.pkl\n",
    "   │     ⋮     ⋮\n",
    "   │     └── nx_20\n",
    "   │          ├── bayes_data.pkl\n",
    "   │          └── norm_bayes_data.pkl\n",
    "   │\n",
    "   ├── freq_data\n",
    "   │     │\n",
    "   │     └── freq_data.pkl\n",
    "   │\n",
    "   └── hmm_data\n",
    "         ├── nx_4\n",
    "         │    ├── hmm_data.pkl\n",
    "         │    └── norm_hmm_data.pkl\n",
    "         ├── nx_6\n",
    "         │    ├── hmm_data.pkl\n",
    "         │    └── norm_hmm_data.pkl\n",
    "         ⋮     ⋮\n",
    "         └── nx_20\n",
    "              ├── hmm_data.pkl\n",
    "              └── norm_hmm_data.pkl\n",
    "   \n",
    "```\n",
    "\n",
    "Here the frequency spectrum approach only has one feature type to be used. But the Hidden Markov Model and the Naive Bayes likelihood has several one, provided the combination of the parameters $n_x$ and the possibility to normalize or not the light curve time series. Thence, if the user want the parameters for the model with $n_x=8$, for the case considering the normalized data, it will be found at `/features/hmm_data/nx_8/norm_hmm_data.pkl` for the hidden markov model approach, and `/features/bayes_data/nx_8/norm_bayes_data.pkl` for the naive bayes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
