

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Welcome to CoRoT Contribution’s documentation! &mdash; CoRoT Contributions</title>
  

  
  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Read .fits raw data" href="01 - Reading and Plotting.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="#" class="icon icon-home"> CoRoT Contributions
          

          
            
            <img src="_static/corot_logo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Reading and Plotting</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="01 - Reading and Plotting.html">Read <em>.fits</em> raw data</a></li>
<li class="toctree-l1"><a class="reference internal" href="01 - Reading and Plotting.html#Preprocessing-data">Preprocessing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="01 - Reading and Plotting.html#Resampling-series">Resampling series</a></li>
<li class="toctree-l1"><a class="reference internal" href="01 - Reading and Plotting.html#Filtering-series">Filtering series</a></li>
<li class="toctree-l1"><a class="reference internal" href="01 - Reading and Plotting.html#Application-example">Application example</a></li>
<li class="toctree-l1"><a class="reference internal" href="01 - Reading and Plotting.html#Generation-algorithms">Generation algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="01 - Reading and Plotting.html#Save-pre-processed-data">Save pre-processed data</a></li>
</ul>
<p class="caption"><span class="caption-text">Feature Engineering</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="02 - Building and creating features.html">Reading the data</a></li>
<li class="toctree-l1"><a class="reference internal" href="02 - Building and creating features.html#Feature:-Frequency-response">Feature: Frequency response</a></li>
<li class="toctree-l1"><a class="reference internal" href="02 - Building and creating features.html#Feature:-Naive-Bayes-likelihood">Feature: Naive Bayes likelihood</a></li>
<li class="toctree-l1"><a class="reference internal" href="02 - Building and creating features.html#Feature:-Markov-Hidden-Models">Feature: Markov Hidden Models</a></li>
</ul>
<p class="caption"><span class="caption-text">Machine Learning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="03 - Machine Learning - XGBoost Classifier.html">XGBoost Classifier</a></li>
<li class="toctree-l1"><a class="reference internal" href="03 - Machine Learning - Decision Trees.html">Decision trees</a></li>
</ul>
<p class="caption"><span class="caption-text">Code APIs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="modules.html">CoRoT Contributions</a></li>
</ul>
<p class="caption"><span class="caption-text">Authors</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="author/vanderlei.html">Vanderlei Cunha Parro </a></li>
<li class="toctree-l1"><a class="reference internal" href="author/marcelo.html">Marcelo Mendes Lafetá Lima </a></li>
<li class="toctree-l1"><a class="reference internal" href="author/roberto.html">Roberto Bertoldo Menezes </a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">CoRoT Contributions</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="#">Docs</a> &raquo;</li>
        
      <li>Welcome to CoRoT Contribution’s documentation!</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="welcome-to-corot-contribution-s-documentation">
<h1>Welcome to CoRoT Contribution’s documentation!<a class="headerlink" href="#welcome-to-corot-contribution-s-documentation" title="Permalink to this headline">¶</a></h1>
<p><a class="reference external" href="https://readthedocs.org/projects/corot-contributions/"><img alt="Documentation Status" src="https://readthedocs.org/projects/corot-contributions/badge/?version=latest" /></a> <a class="reference external" href="https://github.com/lafetamarcelo/CoRoTContributions/"><img alt="GitHub last commit" src="https://img.shields.io/github/last-commit/lafetamarcelo/CoRoTContributions" /></a> <img alt="GitHub top language" src="https://img.shields.io/github/languages/top/lafetamarcelo/CoRoTContributions" /> <a class="reference external" href="https://discord.gg/EenaYE5"><img alt="Join discussion Discord" src="https://img.shields.io/discord/713442259210600448?label=join%20discord" /></a></p>
<p>Here the summary results and developments of the project called A Data Analysis Pipeline
for the CoRoT Light Curve Data, supported by NSEE and FAPESP. The pipeline major three
specific topics that will allow any user with basic knowledge in Python to reproduce the
analysis. The work is called here also by its repository name: <em>CoRoT Contributions</em>.</p>
<p>This work fundamentally rely on multidiscipline knowledge, passing by computation,
mathematics, signal analysis and phisycs. The main idea is to show how to implement this
fields knowledge to solve a practical problem, therefore no introduction level discussion
is expected. The focus relies on high level discution on most of those subjects.</p>
<p>In this documentation first an <strong>Introduction</strong> on the CoRoT mission is presented to bring
the reader more close to the problem faced by the astronomical community and clearify the
importance of the proposed problem. Thence, the <strong>Problem</strong> that will be faced is presented
and discussed to introduce the necessary knowledge to interpret the data and understand some
decisions taken during the project developments. Finally the data processing <strong>Pipeline</strong> is
presented with all the algorithms and analysis necessary to get to the exoplanet classifier
presented as the project results.</p>
<p>Finally, all pipeline steps are presented in a detailed manner, by going through all the
low-level computations, with code examples. Leading the way to the final results obtained,
and reports.</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>This documentation is just a guide trough how one can use the provided python libraries
do go from the raw data collected from the CoRoT satellite, to a simple architecture to be
used on machine learning algorithms. Then, some examples of how to use theses features on
machine learning classifiers are presented. If one also wants to check other possible
applications please check the GitHub repository.</p>
</div>
<div class="section" id="introduction-to-corot">
<h2>Introduction to CoRoT<a class="headerlink" href="#introduction-to-corot" title="Permalink to this headline">¶</a></h2>
<p>The CoRoT (Convection, Rotation and planetary Transits) was the first mission dedicated and
designed for the exoplanetary research. Operated in a lower Earth orbit, has the objectives of
using its CCD widefield cameras to obtain and gather light information for the study of supposed
exoplanetary behaviors. The project, launched in 2006 had nominal lifetime of 2.5 years, but it
actually ended in 2014 when it was de-orbited. The project was led by CNES, with contributions
from ESA, Austria, Belgium, Germany, Spain and Brazil.</p>
<p>One of the key information collected by the CCD cameras was the light intensity from the planets,
which there is a collection of thousands of light curves observations of different candidates.
Interesting enough, this information are actually from exoplanets candidates, and the astronomical
community is now working on classifying those observations to map which ones are actually exoplanets
from those who are not. There are several methods to detect exoplanets and get information from them,
as an example, there are:</p>
<ul class="simple">
<li><p>Radial velocity</p></li>
<li><p>Transit photometry</p></li>
<li><p>Relativistic beaming</p></li>
<li><p>Gravitation microlensing</p></li>
</ul>
<p>and several others. Here, teh transit photometry approach will be used, and a further explanation on
this technique will be presented on the next section. No more will be discussed from the other thecniques,
but the reader is free to check it out how those work. The only interesting subject that most of those
has in common is that their collected information are time series (dynamical information), and most of
the techniques presented in this study could be used similarly for all of them.</p>
<p>For more reference on works regarding the classification of the light curves observations one
might check the following <a class="reference external" href="https://www.aanda.org/articles/aa/pdf/2018/11/aa31068-17.pdf">M. Deleuil1, S. Aigrain2, et al</a>.</p>
<p>For more details on the CoRoT mission please check out the <a class="reference external" href="https://sci.esa.int/web/corot">ESA CoRoT site</a>.</p>
</div>
<div class="section" id="problem-proposed">
<h2>Problem proposed<a class="headerlink" href="#problem-proposed" title="Permalink to this headline">¶</a></h2>
<p>Being as concise as possible, the main goal is to use machine learning to automate
the classification of CoRoT collected data, as a way to cluster data that has meaningful
information to be analyzed from data that has none. This means that we want to classify
data that present possible exo-planets, from data that has information about something
other then exo-planets.</p>
<p>To provide a brief introduction to the knowledge needed to achieve this goal, we will
describe the data set a little bit. The data set is provided by the repository provided
at <a class="reference external" href="http://idoc-corot.ias.u-psud.fr/sitools/client-user/COROT_N2_PUBLIC_DATA/project-index.html">the web site</a> ,
where it contains files in a <em>.fits</em> format. Those are pretty common in the
astronomical whereabouts, but not for the machine learning public. In each <em>.fits</em>
file, we will have a light curve time series for a particular CoRoT observation.
All observations will have the white light intensity, some will have the RGB.</p>
<p>Therefore, we are talking about using the transit photometry time series to classify if
in this particular observation there is a chance for the observation be one of an exo-planet,
or something else. As an illustration, one will have time-series data, that will contain
the planetary transit information such in</p>
<a class="reference internal image-reference" href="_images/planetary_transit.png"><img alt="_images/planetary_transit.png" src="_images/planetary_transit.png" style="width: 800px;" /></a>
<p>So the features for the machine learning algorithm must be gathered from the time series,
and the algorithm must reach for the probability of this observation be the observation
of an exo-planet.</p>
</div>
<div class="section" id="pipeline">
<h2>Pipeline<a class="headerlink" href="#pipeline" title="Permalink to this headline">¶</a></h2>
<p>The preprocessing pipeline consists on four major steps:</p>
<ul class="simple">
<li><p>Getting the data - (10%)</p></li>
<li><p>Preprocessing in Python - (20%)</p></li>
<li><p>Feature Engineering - (50%)</p></li>
<li><p>Machine Leaning - (20%)</p></li>
</ul>
<a class="reference internal image-reference" href="_images/pipeline.png"><img alt="_images/pipeline.png" class="align-center" src="_images/pipeline.png" style="width: 550px;" /></a>
<div class="admonition-notice admonition">
<p class="admonition-title">Notice</p>
<p>In these items, we introduced the usual percentage of the work amount usually taken for
each particular part of a machine learning project. This is usually the amount when you
have features that do not involve time, dynamic features… therefore static information,
which is mostly used to happen, such as in problems of customer clustering, when one has
features such</p>
<table class="hlist"><tr><td><ul class="simple">
<li><p>Age</p></li>
</ul>
</td><td><ul class="simple">
<li><p>Salary</p></li>
</ul>
</td><td><ul class="simple">
<li><p>Address</p></li>
</ul>
</td><td><ul class="simple">
<li><p>…</p></li>
</ul>
</td><td><ul class="simple">
</ul>
</td></tr></table>
<p>Note that these features do not change over time. Here we have a time series, so we have
to somehow extract from those time series, some static information. So be prepared for the
feature engineering part of the pipeline!</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Note that the first three pipeline steps will generate a data file, with the processed data
for the next step. The user can choose to either follow each process step and by it self
generate and preprocess the data. Or one can go the google drive link bellow</p>
<ul class="simple">
<li><p><a class="reference external" href="https://drive.google.com/drive/folders/19kALbQ5m1ppXxGTVMBaWA4KdIE9pmWfM?usp=sharing">Google drive access</a></p></li>
</ul>
<p>and download the already preprocessed data. For example, if the user wants to jump all
the three first steps and just have access to the features for the machine learning
algorithm, one can download then from the folder <cite>/features</cite> in the google drive.</p>
</div>
<a class="reference internal image-reference" href="_images/progress_one.png"><img alt="_images/progress_one.png" src="_images/progress_one.png" style="width: 800px;" /></a>
<p>To get the data on your local machine, first one mush address to the
<a class="reference external" href="http://idoc-corot.ias.u-psud.fr/sitools/client-user/COROT_N2_PUBLIC_DATA/project-index.html">data repository web site</a>
where we will have the soo called level N2 processed data from the CoRoT observations.
Those are the most processed versions of the CoRoT collected data. At this repository,
one will find five classes of observations,</p>
<ul class="simple">
<li><p>CoRoT bright: the called bright stars (examples of not exo-planets)</p></li>
<li><p>CoRoT targets: the called exo-planets (examples of exo-planets)</p></li>
<li><p>Red giants: the observations classified as red giants (examples of not exo-planets)</p></li>
<li><p>Stars with rotation periods from Affer (not used)</p></li>
<li><p>Stars with rotation periods from Medeiros (not used)</p></li>
</ul>
<p>The last two classes will not interest us. We actually will only use the first three
collection of data. Those collections have information of transit photometry or only
light curve intensity, furthermore their labels allow us to segregate the information
as not exo-planet and exo-planet. While the last two, provide labels that cannot ensure
if the collected data is from an exo-planet observation or not.</p>
<p>If the user select one of the three categories, it will show a table with several items.
Each item is a particular light curve observation, and the user can select and download
any of the curves that he desires.</p>
<p>After that one might also download another class of light curve observations called
eclipsing binaries from the <a class="reference external" href="http://idoc-corot.ias.u-psud.fr/sitools/client-user/COROT_N2_PUBLIC_DATA/project-index.html">data repository</a>
in the Query Forms tab in the FAINT STARS option. There the user will be able to query
each curve from their specific CoRoT Id. Then by searching the stars CoRoT Id from the
tables at the <a class="reference external" href="http://cdsarc.u-strasbg.fr/viz-bin/qcat?J/A+A/619/A97">CoRoT transit catalog</a>,
it is possible to reach a group of close to 1500 eclipsing binaries.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>One interesting aspect of the eclipsing binaries is that their light curves are pretty
close to the exo-planets ones. Therefore it is pretty difficult to cluster eclipsing
binaries from exo-planets using only the light curve. This is the most interesting
case study of this project.</p>
</div>
<a class="reference internal image-reference" href="_images/process_download.png"><img alt="_images/process_download.png" src="_images/process_download.png" style="width: 800px;" /></a>
<p>After downloading for the three classes (bright stars, confirmed targets and red giants)
the user must keep each class in a particular folder, with the name that the user wants
to label each class. As schematized in the figure above.</p>
<p>It is advised to keep a database folder following the structure:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span>./database
   │
   ├── bright_stars
   │     ├── ..._1.fits
   │     ├── ..._2.fits
   │     ⋮       ⋮
   │     └── ..._k.fits
   │
   ├── confirmed_targets
   │     ├── ..._1.fits
   │     ├── ..._2.fits
   │     ⋮       ⋮
   │     └── ..._l.fits
   │
   ├── eclipsing_binaries
   │     ├── ..._1.fits
   │     ├── ..._2.fits
   │     ⋮       ⋮
   │     └── ..._m.fits
   │
   └── red_giants
         ├── ..._1.fits
         ├── ..._2.fits
         ⋮       ⋮
         └── ..._n.fits
</pre></div>
</div>
<div class="admonition-drive-files-access admonition">
<p class="admonition-title">Drive files access</p>
<p>If the user does not want to go through the process presented above and just want to
download the data, already in the format presented above, it is possible to get it
from my google drive, or in the direct download link:</p>
<table class="hlist"><tr><td><ul class="simple">
<li><p><a class="reference external" href="https://drive.google.com/drive/folders/19kALbQ5m1ppXxGTVMBaWA4KdIE9pmWfM?usp=sharing">Google drive access</a></p></li>
</ul>
</td><td><ul class="simple">
<li><p><a class="reference external" href="https://drive.google.com/uc?export=download&amp;id=ttps://drive.google.com/drive/folders/19kALbQ5m1ppXxGTVMBaWA4KdIE9pmWfM?usp=sharin">Direct download</a></p></li>
</ul>
</td></tr></table>
<p>The files are something close to tens of GB. Since it contains all the raw fits files
for the three classes (bright stars, red giants, confirmed targets and eclipsing binaries).</p>
</div>
<a class="reference internal image-reference" href="_images/progress_two.png"><img alt="_images/progress_two.png" src="_images/progress_two.png" style="width: 800px;" /></a>
<p>After getting the data from the online repositories, we need to transform this data into data
structures more python friendly, such as dictionaries, lists, arrays, or into files with more
suitable formats then <em>fits</em> files, <em>e.g.</em> pickle files, or MATLAB files (if one wants to
algorithms on MATLAB).</p>
<p>For that, we create here a preprocessing library called <a class="reference internal" href="utils.html#utils.data_helper.Reader" title="utils.data_helper.Reader"><code class="xref py py-class docutils literal notranslate"><span class="pre">Reader</span></code></a>
that transform the data from the fits files into simple objects of the class <a class="reference internal" href="utils.html#utils.data_struc.Curve" title="utils.data_struc.Curve"><code class="xref py py-class docutils literal notranslate"><span class="pre">Curve</span></code></a>
that simplifies the information access, and make it faster, provided that it works with the so-called
memory maps and only save HUD tables that will be necessary for the future. This process reduces the
dataset up to 80% (the data that was something close to 40 GB, is now something close to 7 GB).</p>
<p>The next scheme illustrates the process applied to the data to create this interface from the <em>.fits</em>
format into the python friendly form:</p>
<a class="reference internal image-reference" href="_images/process_reading.png"><img alt="_images/process_reading.png" src="_images/process_reading.png" style="width: 800px;" /></a>
<p>From the above scheme, we can see that the process starts from the <em>fits</em> files and after passing
through the <a class="reference internal" href="utils.html#utils.data_helper.Reader" title="utils.data_helper.Reader"><code class="xref py py-class docutils literal notranslate"><span class="pre">Reader</span></code></a>, we achieve a structure that with simple calls
to the information variables, it is possible to get that variable data in <cite>dict</cite> and/or <cite>list</cite> formats.</p>
<p>After reading the data, and before saving in a format such as the pickle file, we also apply a preprocessing
technique to filter out the noise of the time series. This technique is applied here instead of in the
feature engineering step, just to show the user the importance to understand the feature processing part
with an information extraction practical example. Showing that a simple filtering technique can enable access
to several other pieces of information. And by that, engaging the user to see the next step of the pipeline,
which will have several other feature engineering techniques.</p>
<a class="reference internal image-reference" href="_images/progress_three.png"><img alt="_images/progress_three.png" src="_images/progress_three.png" style="width: 800px;" /></a>
<p>The feature engineering process is the part that takes most work in a machine learning project. It is the
part where one must extract meaningful information from the data to provide for the machine learning
algorithms. But interesting enough, one cannot know beforehand what information is actually the best one
for the machine learning algorithm. Most machine learning projects, the data is actually static (does
not depend on time) and have a simpler path to be taken, where the only usual procedure is done in the
feature engineering is the common normalization and feature reduction (when the dimension of the features
are very high and the learning algorithm is pretty heavy). But in the case of time series classification,
the user has dynamic data… which needs to be transformed to static data, to then go through the common
feature engineering processes.</p>
<p>There are several paths that one might take to get static information from the data when one has a time
series in the hands. Some of the techniques are presented:</p>
<ul class="simple">
<li><p>Power spectrum</p></li>
<li><p>Periodograms</p></li>
<li><p>Number of peaks</p></li>
<li><p>Dynamic insights</p></li>
</ul>
<p>The first two, are not much than the Fourier representation of the data, which is actually the most
characteristic information of the time series. The second is just commonly used info in the internet
tutorials (which is a very noisy scenario, which will only generate more noisy data). The last one usually
is a very powerful approach, in which, we first use the data to estimate a dynamical model of the time
series phenomenon, and then use that model parameter to cluster each time series. The most interesting
part of this approach is that it relies on the leaning algorithm focus to enhance certain behaviors of
the dynamic model and mitigate the effects of others.</p>
<p>Usually, people abuse simple least-squares or deterministic techniques to acquire the parameters of a
particular structure model. Here we will present a stochastic approach of this technique, where we will
estimate stochastic and probabilistic models for the time series, and their parameters will then be used
as features for the machine learning process.</p>
<p>Here the bellow features will be generated for the machine learning algorithm:</p>
<ul class="simple">
<li><p>Periodograms</p></li>
<li><p>Bayes marginal likelihood</p></li>
<li><p>Hidden Markov Models</p></li>
<li><p>Latent Dirichlet Allocation</p></li>
</ul>
<p>The first one is the classical technique (which present several concerns and computation problems, due
to the Fourier algorithms). The second one uses probabilistic Bayes Models parameters as features. The
third uses the estimated parameters for Hidden Markov Models as features. And the fourth one actually
is a very complex technique usually used to cluster text into speech topics, and here is used outside
its scope, to check if the topic’s classification also can be applied to time series dependencies.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Provided that each approach is more unique than the other, here it will not be discussed further more
about any one of them. A more detailed description will be presented in the Feature Engineering step
of the documentation.</p>
</div>
<a class="reference internal image-reference" href="_images/progress_four.png"><img alt="_images/progress_four.png" src="_images/progress_four.png" style="width: 800px;" /></a>
<p>The machine learning step of the pipeline, does nothing much more than the basics machine learning procedures
once the time series static information is provided. From the last step, simple features where build from the
time series. using the following techniques:</p>
<ul class="simple">
<li><p>Periodograms: produces the evenly reshaped power spectrums</p></li>
<li><p>Bayes marginal likelihood: produces the Ridge Bayesian model parameters</p></li>
<li><p>Hidden Markov Models: produces the Markov trasition matrices</p></li>
</ul>
<p>And now it is desired to build a machine learning algorithm that will take each one of those features and try
to classify the observation classes. Therefore, we just want to distinguish between the four labels:</p>
<ul class="simple">
<li><p>Red giants</p></li>
<li><p>Confirmed targets or exo-planets</p></li>
<li><p>Bright stars</p></li>
<li><p>Eclipsing binaries</p></li>
</ul>
<p>The astronomical community show a particular interest in distinguish specifically two of those classes from each
other: the confirmed targets from the eclipsing binaries. This challenge show itself to be appealing because there
is several difficulties in distinguishing the eclipsing binaries and exo planets light curve information using
classical theory. Therefore, having an automate algorithm capable of solving this enigma would be most attractive.</p>
<p>Thence here we present three major algorithms to classify the data from exo-planets and eclipsing binaries. The
first is using the <a class="reference external" href="https://xgboost.readthedocs.io/en/latest/">XGBoost library</a>, since it is proven to be a
very powerful algorithm in machine learning forums and competitions, such as <a class="reference external" href="https://www.kaggle.com/">Kaggle</a>.
Also, some classic algorithms will be used, such as Decision Trees from the <a class="reference external" href="https://scikit-learn.org/stable/">scikit learn</a>
packages to compare the boosting feature capability from the XGBoost algorithms, and some powerful mathematical
algorithms such as Support Vector Machines classifiers, which is supposed to be analytically optimal.</p>
<p>All the algorithm will follow the common machine learning path, such as:</p>
<ul class="simple">
<li><p>Read the features and labels</p></li>
<li><p>Encode, normalize, reduce features and reshape</p></li>
<li><p>Make train and test data-set</p></li>
<li><p>Build the machine learning model</p></li>
<li><p>Search hyper parameters</p></li>
<li><p>Train machine learning model</p></li>
<li><p>Validate the model</p></li>
<li><p>Analyse the results</p></li>
</ul>
<p>So the reader can understand that for any of the models and any of the features, after the Feature Engineering
pipeline, the algorithm is pretty much the same. Therefore, the price of dealing with time series data, is that
one must have the Feature Engineering pipeline to extract static information from the data, before going through
the common machine learning pipeline.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As in the previous pipeline step, the algorithms will not be further discussed here. If the reader wants, a
more detailed description of each machine learning approach can be found on its respective chapter of the
documentation, with application example.</p>
</div>
</div>
<div class="section" id="some-results">
<h2>Some results…<a class="headerlink" href="#some-results" title="Permalink to this headline">¶</a></h2>
<p>To show the capability of the approach presented, checkout the some of the best results obtained with the XGBoost
Classifier algorithm:</p>
<a class="reference internal image-reference" href="_images/xgb_results.png"><img alt="_images/xgb_results.png" class="align-center" src="_images/xgb_results.png" style="width: 300px;" /></a>
<p>In this figure, the confusion matrix of the classifier predictions for the testing dataset is presented. Notice
that the XGBoost algorithm, was able to provide a <strong>93%</strong> accuracy on <strong>exoplanet labels</strong>, and <strong>83%</strong> on the
<strong>eclipsing binaries</strong>, as shown by the above confusion matrix.</p>
<div class="toctree-wrapper compound">
<p class="caption"><span class="caption-text">Reading and Plotting</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="01 - Reading and Plotting.html">Read <em>.fits</em> raw data</a></li>
<li class="toctree-l1"><a class="reference internal" href="01 - Reading and Plotting.html#Preprocessing-data">Preprocessing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="01 - Reading and Plotting.html#Resampling-series">Resampling series</a><ul>
<li class="toctree-l2"><a class="reference internal" href="01 - Reading and Plotting.html#Sample-time-distribution">Sample time distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="01 - Reading and Plotting.html#Resample-time-series-data">Resample time series data</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="01 - Reading and Plotting.html#Filtering-series">Filtering series</a></li>
<li class="toctree-l1"><a class="reference internal" href="01 - Reading and Plotting.html#Application-example">Application example</a></li>
<li class="toctree-l1"><a class="reference internal" href="01 - Reading and Plotting.html#Generation-algorithms">Generation algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="01 - Reading and Plotting.html#Save-pre-processed-data">Save pre-processed data</a><ul>
<li class="toctree-l2"><a class="reference internal" href="01 - Reading and Plotting.html#Save-as-.mat-file">Save as <em>.mat</em> file</a></li>
<li class="toctree-l2"><a class="reference internal" href="01 - Reading and Plotting.html#Save-as-.pickle-file">Save as <em>.pickle</em> file</a></li>
</ul>
</li>
</ul>
</div>
<div class="toctree-wrapper compound">
<p class="caption"><span class="caption-text">Feature Engineering</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="02 - Building and creating features.html">Reading the data</a></li>
<li class="toctree-l1"><a class="reference internal" href="02 - Building and creating features.html#Feature:-Frequency-response">Feature: Frequency response</a><ul>
<li class="toctree-l2"><a class="reference internal" href="02 - Building and creating features.html#Introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="02 - Building and creating features.html#Spectrum-generation">Spectrum generation</a></li>
<li class="toctree-l2"><a class="reference internal" href="02 - Building and creating features.html#Detrended-spectrum">Detrended spectrum</a></li>
<li class="toctree-l2"><a class="reference internal" href="02 - Building and creating features.html#Resample-spectrum">Resample spectrum</a></li>
<li class="toctree-l2"><a class="reference internal" href="02 - Building and creating features.html#Generation-algorithm">Generation algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="02 - Building and creating features.html#Save-feature">Save feature</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="02 - Building and creating features.html#Feature:-Naive-Bayes-likelihood">Feature: Naive Bayes likelihood</a><ul>
<li class="toctree-l2"><a class="reference internal" href="02 - Building and creating features.html#Regression-model">Regression model</a></li>
<li class="toctree-l2"><a class="reference internal" href="02 - Building and creating features.html#Next-step-parameters">Next step parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="02 - Building and creating features.html#id1">Save feature</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="02 - Building and creating features.html#Feature:-Markov-Hidden-Models">Feature: Markov Hidden Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="02 - Building and creating features.html#Preprocessing-data">Preprocessing data</a></li>
<li class="toctree-l2"><a class="reference internal" href="02 - Building and creating features.html#Estimate-HMM">Estimate HMM</a></li>
<li class="toctree-l2"><a class="reference internal" href="02 - Building and creating features.html#id2">Save feature</a></li>
<li class="toctree-l2"><a class="reference internal" href="02 - Building and creating features.html#Download-features">Download features</a></li>
</ul>
</li>
</ul>
</div>
<div class="toctree-wrapper compound">
<p class="caption"><span class="caption-text">Machine Learning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="03 - Machine Learning - XGBoost Classifier.html">XGBoost Classifier</a><ul>
<li class="toctree-l2"><a class="reference internal" href="03 - Machine Learning - XGBoost Classifier.html#Periodograms">Periodograms</a></li>
<li class="toctree-l2"><a class="reference internal" href="03 - Machine Learning - XGBoost Classifier.html#Naive-Bayes-likelihood">Naive Bayes likelihood</a></li>
<li class="toctree-l2"><a class="reference internal" href="03 - Machine Learning - XGBoost Classifier.html#Hidden-Markov-Models">Hidden Markov Models</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="03 - Machine Learning - Decision Trees.html">Decision trees</a><ul>
<li class="toctree-l2"><a class="reference internal" href="03 - Machine Learning - Decision Trees.html#Periodograms">Periodograms</a></li>
<li class="toctree-l2"><a class="reference internal" href="03 - Machine Learning - Decision Trees.html#Naive-Bayes-likelihood">Naive Bayes likelihood</a></li>
<li class="toctree-l2"><a class="reference internal" href="03 - Machine Learning - Decision Trees.html#Hidden-Markov-Models">Hidden Markov Models</a></li>
</ul>
</li>
</ul>
</div>
<div class="toctree-wrapper compound">
<p class="caption"><span class="caption-text">Code APIs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="modules.html">CoRoT Contributions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="utils.html">utils Package</a></li>
</ul>
</li>
</ul>
</div>
<div class="toctree-wrapper compound">
<p class="caption"><span class="caption-text">Authors</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="author/vanderlei.html">Vanderlei Cunha Parro </a></li>
<li class="toctree-l1"><a class="reference internal" href="author/marcelo.html">Marcelo Mendes Lafetá Lima </a></li>
<li class="toctree-l1"><a class="reference internal" href="author/roberto.html">Roberto Bertoldo Menezes </a></li>
</ul>
</div>
<h3 style="text-align:center;">
   <iframe
      src="https://discordapp.com/widget?id=713442259210600448&theme=dark"
      width="500"
      height="300"
      allowtransparency="true"
      frameborder="0"
   ></iframe>
</h3></div>
</div>
<div class="section" id="indices-and-tables">
<h1>Indices and tables<a class="headerlink" href="#indices-and-tables" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></p></li>
<li><p><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></p></li>
<li><p><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></p></li>
</ul>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="01 - Reading and Plotting.html" class="btn btn-neutral float-right" title="Read .fits raw data" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Marcelo Lima

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>