{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read _.fits_ raw data\n",
    "\n",
    "---\n",
    "\n",
    "For reading the *.fits* data-set one may use the utils library. From the `utils` library, there is a `data_helper.Reader()` object, optimized to read the *.fits* files. Inside a *.fits* file, usually is found a sctructure with three tables of type HUD. In this particular case, the three tables teels a *history* of the data, where the first is the most raw data possible, then the second is the treated/filtered version of the first table, and the third table is the compressed version of the information from the second table.\n",
    "\n",
    "> *We are particularly interested in the most informative data-set in a minimal (compressed) manner. Also, here we do not have the luxury of dealing with big-data problems... for that we might need **Spark** or **Hadoop** support. Thence, the third table is usually the most interesting one.*\n",
    "\n",
    "All files from a given folder will be readed and labeled as disered. As an example an advised folder structure is the following:\n",
    "\n",
    "```Json\n",
    "./database\n",
    "   │\n",
    "   ├── bright_stars\n",
    "   │     ├── ..._1.fits\n",
    "   │     ├── ..._2.fits\n",
    "   │     ⋮       ⋮\n",
    "   │     └── ..._k.fits\n",
    "   │\n",
    "   ├── confirmed_targets\n",
    "   │     ├── ..._1.fits\n",
    "   │     ├── ..._2.fits\n",
    "   │     ⋮       ⋮\n",
    "   │     └── ..._l.fits\n",
    "   │\n",
    "   ├── eclipsing_binaries\n",
    "   │     ├── ..._1.fits\n",
    "   │     ├── ..._2.fits\n",
    "   │     ⋮       ⋮\n",
    "   │     └── ..._m.fits\n",
    "   │\n",
    "   └── red_giants\n",
    "         ├── ..._1.fits\n",
    "         ├── ..._2.fits\n",
    "         ⋮       ⋮\n",
    "         └── ..._n.fits\n",
    "```\n",
    "\n",
    "Inside the `database/bright_stars` it has all *.fits* files of class *bright stars*, in `database/confirmed_targets` it has all *.fits* files of class *confirmed exoplanets*, in `database/eclipsing binaries` it has all *.fits* files of class *confirmed multi transit eclipsing binaries* and in `database/red_giants` it has all *.fits* files of class *red giants*. \n",
    "\n",
    "All the provided folders are then readed and the data is concatenated into one big list of `data_struc.Curve` objects, wich is a pretty helpful interface structure from the `astropy.table` objects to simple `list` and `dict` variables. Thence, in the variable `curves` we actually have a `list` of `data_struc.Curve`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T19:13:34.087195Z",
     "start_time": "2020-02-20T19:13:29.842631Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.0.1.min.js\": \"JpP8FXbgAZLkfur7LiK3j9AGBhHNIvF742meBJrjO2ShJDhCG2I1uVvW+0DUtrmc\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.0.1.min.js\": \"xZlADit0Q04ISQEdKg2k3L4W9AwQBAuDs9nJL9fM/WwzL1tEU9VPNezOFX0nLEAz\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.0.1.min.js\": \"4BuPRZkdMKSnj3zoxiNrQ86XgNw0rYmBOxe7nshquXwwcauupgBF2DHLVG1WuZlV\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.0.1.min.js\": \"Dv1SQ87hmDqK6S5OhBf0bCuwAEvL5QYL0PuR/F1SPVhCS/r/abjkbpKDYL2zeM19\"};\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.0.1.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.0.1.min.js\": \"JpP8FXbgAZLkfur7LiK3j9AGBhHNIvF742meBJrjO2ShJDhCG2I1uVvW+0DUtrmc\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.0.1.min.js\": \"xZlADit0Q04ISQEdKg2k3L4W9AwQBAuDs9nJL9fM/WwzL1tEU9VPNezOFX0nLEAz\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.0.1.min.js\": \"4BuPRZkdMKSnj3zoxiNrQ86XgNw0rYmBOxe7nshquXwwcauupgBF2DHLVG1WuZlV\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.0.1.min.js\": \"Dv1SQ87hmDqK6S5OhBf0bCuwAEvL5QYL0PuR/F1SPVhCS/r/abjkbpKDYL2zeM19\"};\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.0.1.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:reader_log:Reader module created...\n",
      "INFO:reader_log:Reading 37 curve packages...\n",
      "INFO:reader_log:Done reading!\n",
      "INFO:reader_log:Reading 40 curve packages...\n",
      "INFO:reader_log:Done reading!\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "folder_list = [ './database/raw_fits/confirmed_targets',\n",
    "                './database/raw_fits/eclipsing_binaries',\n",
    "                './database/raw_fits/red_giants',\n",
    "                './database/raw_fits/bright_stars' ]\n",
    "\n",
    "dread = data_helper.Reader()\n",
    "curves = dread.from_folder(folder=folder_list[0], label='confirmed targets', index=2)\n",
    "curves += dread.from_folder(folder=folder_list[1], label='eclipsing binaries', index=2, items=40)\n",
    "#curves += dread.from_folder(folder=folder_list[2], label='red giants', index=2)\n",
    "#curves += dread.from_folder(folder=folder_list[3], label='bright stars', index=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the `utils` as shown, one is actually import all the following packages:\n",
    "\n",
    "- `data_helper` with Reader Object\n",
    "- `data_struc` with Curve Object\n",
    "- `visual` with several plot functions\n",
    "\n",
    "And as an example, the visual package just compress the bokeh code library, since it is a pretty expansive code, for example, to make a line plot without visual, one should do something like\n",
    "\n",
    "```Python\n",
    "from bokeh.palettes import Magma\n",
    "from bokeh.layouts import column\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.io import output_notebook, push_notebook\n",
    "\n",
    "p = figure( \n",
    "    title=\"Some title\",\n",
    "    plot_width=400,\n",
    "    plot_height=600)\n",
    "    \n",
    "# Style the figure image\n",
    "p.grid.grid_line_alpha = 0.1\n",
    "p.xgrid.band_fill_alpha = 0.1\n",
    "p.xgrid.band_fill_color = Magma[10][1]\n",
    "p.yaxis.axis_label = \"Some label for y axis\"\n",
    "p.xaxis.axis_label = \"Some label for x axis\"\n",
    "\n",
    "# Place the information on plot\n",
    "p.line(x_data, y_data,\n",
    "        legend_label=\"My legend label\",\n",
    "        line_width=2,\n",
    "        color=Magma[10][2],\n",
    "        muted_alpha=0.1,\n",
    "        line_cap='rounded')\n",
    "p.legend.location = \"right_top\"\n",
    "p.legend.click_policy = \"disable\"\n",
    "\n",
    "show(p)\n",
    "```\n",
    "\n",
    "and the same can be achieved using `visual`, just as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = visual.line_plot(curves[25][\"DATE\"], \n",
    "                     curves[25]['WHITEFLUXSYS'],\n",
    "                     legend_label='Raw Light Curve', \n",
    "                     title='Example curve plot',\n",
    "                     y_axis={'label': 'Intensity'},\n",
    "                     x_axis={'type': 'datetime',\n",
    "                             'label': 'Date'})\n",
    "\n",
    "visual.show_plot(p)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bokeh_plot.png](./images/nb_gallery/bokeh_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data\n",
    "\n",
    "---\n",
    "\n",
    "The preprocessing part of this analysis will include the preparation of the light curve data for each observation and saving both the original data, such as the preprocessed data in a more suitable data format for Python users. The main goal is to read the data from the *.fits* file (already done with `data_helper.Reader()`), filter the data to remove dynamical noise, and than save in a suitable format such as *.pickle*, or *.mat* for those in favor...\n",
    "\n",
    "Here is the step where we will filter the data. To do that, first we must choose between the two possible paths\n",
    "\n",
    "- Continuous time filters\n",
    "- Discrete time filters\n",
    "\n",
    "> *A practice advice, it is always preferable discrete time filtering, because the routines are simpler and more efficient... but this is not always possible when dealing with several time series. Fortunately, here it is possible! To go with the discrete approach, we must check if all time series has a close mean sample time, with low variance. If not, we must first resample the time series, and then use discrete filtering techniques on the data. Do not try, to apply discrete filtering techniques on analog data (without a consistent sample time), otherwise you will apply a technique that is not a linear transfomation on the data, and therefore, you will be, by hand, introducing noise to the data.*\n",
    "\n",
    "So first, lets check the sample time of the series, and after filter the high frequency noise from the data. The usefulness of the filtering of the time series, will be shown in the final of the document, where a feature extraction technique will be applied just as an example on how the denoised time series is so necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample time distribution\n",
    "\n",
    "To analyse the time sample of the series, one can do a box plot for each time-series curve. At each box plot it is represented the distribution of the diference `t[k] - t[k-1]` for `k` representing each sample of the time-series and `t` the sampled time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T19:14:02.747527Z",
     "start_time": "2020-02-20T19:13:44.982421Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "item = 0\n",
    "values, labels = [], []\n",
    "for curve in curves:\n",
    "    diff_time = np.diff(curve[\"DATE\"])\n",
    "    values += [x.total_seconds() / 60 for x in diff_time]\n",
    "    labels += [str(item) for x in diff_time]\n",
    "    item += 1\n",
    "                                #   ╔══════ Include comment here\n",
    "                                #   ║       to enhance the outliers\n",
    "p = visual.box_plot(values, labels,   y_range=(8.515, 8.555),\n",
    "                    title='Average sample time within curves',\n",
    "                    y_axis={'label': 'Sample time (min)'},\n",
    "                    x_axis={'label': 'Curves'})\n",
    "visual.show_plot(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sample time distribution](./images/nb_gallery/bokeh_plot(1).png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T01:50:14.045591Z",
     "start_time": "2020-02-15T01:50:14.039605Z"
    }
   },
   "source": [
    "## Resampling time series\n",
    "\n",
    "Since the time sample time do not vary that much from one light curve to another, **not considering the outliers**... One can create the sample time as the mean of the median and then resample each time series using this found value.\n",
    "\n",
    "> _One also might say that those time series, does not need resampling, since all the series present a close sample time mean. But notice that the above figure has the `y` axis clipped from `[8.515, 8.555]`. If the user comment the line described on the above cell, one will see that there are some worrying outliers. **It is because of those outliers that a resampling technique must be applied!**_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T19:14:15.126812Z",
     "start_time": "2020-02-20T19:14:15.082929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time sample is now 8.55 minutes.\n"
     ]
    }
   ],
   "source": [
    "import statistics as stt\n",
    "\n",
    "time_sample = round(stt.mean(values), 2)\n",
    "\n",
    "print(\"The time sample is now {} minutes.\".format(time_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a feasible approximation of the time sample, it is possible to use this reference to reseample each time series to this single sample rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T19:14:26.971545Z",
     "start_time": "2020-02-20T19:14:16.090549Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.signal as scs\n",
    "\n",
    "data = {'y':[], 't':[], 'ti':[], 'lab':[]}\n",
    "for curve in curves:\n",
    "    # Get the time information\n",
    "    init_time = curve['DATE'][0]\n",
    "    data_vector = curve['WHITEFLUXSYS']\n",
    "    time_vector = [(t - init_time).total_seconds() / 60 for t in curve['DATE']]\n",
    "    # Compute the amount of points\n",
    "    n_points = int(time_vector[-1] // time_sample)\n",
    "    # Compute the resampled time series\n",
    "    res_data, res_time = scs.resample(data_vector, n_points, time_vector)\n",
    "    data['y'].append( res_data )\n",
    "    data['t'].append( res_time )\n",
    "    data['ti'].append(init_time)\n",
    "    data['lab'].append(curve.labeler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if the new time sample was correctly placed and there is no more samples with outlier time samples. One can use the histogram of the time sample variation of all light curves to ensure the consistency of the sample time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T19:15:47.974129Z",
     "start_time": "2020-02-20T19:15:45.131708Z"
    }
   },
   "outputs": [],
   "source": [
    "t_std = [stt.stdev(np.diff(t)) for t in data['t']]\n",
    "hist, edges = np.histogram(t_std, density=True, bins=3)\n",
    "\n",
    "p = visual.hist_plot(hist, edges,\n",
    "                    title='Sample time consistency',\n",
    "                    y_axis={'label': 'Density'},\n",
    "                    x_axis={'label': 'Sample time difference (min)'})\n",
    "\n",
    "visual.show_plot(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Distribution concentrated](./images/nb_gallery/bokeh_plot(2).png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Filtering time series\n",
    "\n",
    "The main goal here is to remove the random signals that are contributing to the time series information, the objective is actully to clean the time series from highly spread random variables. This is pretty interesting, because the applied filtering technique will not disturb the meaningful information of the series, since it will only mitigate the random information effects on the series. So lets see the time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T19:16:19.908038Z",
     "start_time": "2020-02-20T19:16:19.867147Z"
    }
   },
   "outputs": [],
   "source": [
    "index = 5         # Curve index\n",
    "\n",
    "p = visual.line_plot(data['t'][index], \n",
    "                     data['y'][index],\n",
    "                     legend_label='Raw Light Curve', \n",
    "                     title='Light Curve',\n",
    "                     y_axis={'label': 'Intensity'},\n",
    "                     x_axis={'type': 'datetime',\n",
    "                             'label': 'Date'})\n",
    "visual.show_plot(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Light curve example](./images/nb_gallery/bokeh_plot(3).png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see that the data is too noisy, from the low variant part of the signal... To reduce the amount of noise on data we might use the `PyAstronomy` library that has some interesting smoothing algorithms, *e.g.* the *hamming* filter that will be used following. We can also change the window size considered for filtering the light curve, here as an example we are using ` window = 33` samples.\n",
    "\n",
    "> _Note that we are correcting the grammar and talking about smothing and not just filtering the data. In smothing techniques, it is spected that the provided data is in a discrete format. Because, actually, a mathematical filtering is applied... not a frequency filter. We are trying to reduce the influence of random variables with highly spread frequency behavior from the data._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T19:16:32.853723Z",
     "start_time": "2020-02-20T19:16:32.811834Z"
    }
   },
   "outputs": [],
   "source": [
    "from PyAstronomy import pyasl\n",
    "\n",
    "window = 33\n",
    "algorithm = 'hamming'\n",
    "sm1 = pyasl.smooth(data['y'][index], window, algorithm)\n",
    "\n",
    "p = visual.line_plot(data['t'][index], \n",
    "                     sm1,\n",
    "                     legend_label='Raw Light Curve', \n",
    "                     title='Light Curve',\n",
    "                     y_axis={'label': 'Intensity'},\n",
    "                     x_axis={'label': 'Time index'})\n",
    "visual.show_plot(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Light curve filtered](./images/nb_gallery/bokeh_plot(4).png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we are talking!! Without the noise is possible to observe the actual variation of the time series, when the eclipses appear. To see the importance of the filtering process applied above, let's try to do some feature engineering... and use the data to extract some information of the process. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Application example\n",
    "\n",
    "We know, that the series from `database/confirmed_targets` are series that highly represents the transit photometry, since it has pronunced eclipses. So let's try, to use the time series data to create an algorithm to determine the moments of eclipse on the time series.\n",
    "\n",
    "The main idea is to create a binary label for the eclipse pattern from the light curve. For that, we could check the light curve derivative to analyse the time series variation along time. Usually when you have a high variation, the chances of having a eclipse is bigger. As an example, let's plot the derivative of one particular light curve, `index = 2`. So for that, lets plot both the derivative of the non filtered time series, and the filtered one, to see if we can take anything out of these informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = visual.line_plot(data['t'][index][1:], \n",
    "                      np.diff(data['y'][index])/time_sample,\n",
    "                      legend_label='Derivative of Light Curve', \n",
    "                      title='Light Curve Derivative',\n",
    "                      color_index=4,\n",
    "                      y_axis={'label': 'Intensity'},\n",
    "                      x_axis={'type': 'datetime',\n",
    "                             'label': 'Date'})\n",
    "p1 = visual.line_plot(data['t'][index][1:], \n",
    "                      np.diff(sm1)/time_sample,\n",
    "                      legend_label='Derivative of Light Curve', \n",
    "                      title='Light Curve Derivative',\n",
    "                      color_index=4,\n",
    "                      y_axis={'label': 'Intensity'},\n",
    "                      x_axis={'label': 'Time index'})\n",
    "visual.show_plot(p, p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Light curve derivative](./images/nb_gallery/bokeh_plot(5).png)\n",
    "![Filtered light curve derivative](./images/nb_gallery/bokeh_plot(6).png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these graphs it is possible to see, that with a simple threshold selection it is possible to infer the regions where we have the eclipses, in the filtered derivative version. \n",
    "\n",
    "> *More practice guide... When dealing with time series, the most simple linear transformation (e.g. a derivative) could enhance the data noise in a very powerfull way. Interesting enough, the noise didn't seem to be that big on the time series plot before right?! So after appling a simple linear trasnformation, the noise increased to be bigger to the biggest actual value of the data. Note the amplidutes of the highest value on the filtered version, and the not filtered one... in the not filtered version, one can only see noise!!* \n",
    ">> *So imagine if you pass this not filtered data trough a complex neural network, that will apply several non linear transformation to your data... we are talking about CAOS! Even if you think you have interesting results, your algorithm is actually working in a very sensible place, that eventually he will go unstable.*\n",
    "\n",
    "> *Another topic to realise... the most treatening noise that one can have in a time series is this called white noise. This noise exists in all frequencies and only statistical and mathematical techniques that obey the law of large numbers can deal with it. And for that, you must have discrete time data!*\n",
    "\n",
    "Then, to introduce some statistics perspective to the result, and make it more automatic, the mean and standard deviation of the light curve variation is used to determine possible decision thresholds, that could infer the moments of initialization of the eclipse, and the finalization...\n",
    "\n",
    "For that, lets compute the mean of the derivative and the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T19:16:37.336588Z",
     "start_time": "2020-02-20T19:16:37.330603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The varaition signal has a mean around 0.035\n",
      "And a standard deviation around 8.41\n"
     ]
    }
   ],
   "source": [
    "variation_mean = np.average(np.diff(sm1)/time_sample)\n",
    "variation_stan = np.std( np.diff(sm1)/time_sample )\n",
    "\n",
    "print(\"The varaition signal has a mean around {}\".format(round(variation_mean,3)))\n",
    "print(\"And a standard deviation around {}\".format(round(variation_stan,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, one can create a threshold close to $\\pm \\sigma$, $\\pm 2\\sigma$ and $\\pm 3\\sigma$, as one can see in the next bellow figure. This thresholds will inform if there was a big varaiation of the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T19:16:46.554558Z",
     "start_time": "2020-02-20T19:16:46.506622Z"
    }
   },
   "outputs": [],
   "source": [
    "size = data['t'][index][1:].shape[0]\n",
    "one_std = variation_stan * np.ones((size,))\n",
    "\n",
    "x_data = data['t'][index][1:]\n",
    "y_data = [np.diff(sm1) / time_sample, \n",
    "          1*one_std, 2*one_std, 3*one_std, -1*one_std, -2*one_std, -3*one_std]\n",
    "legends= ['Derivative', '68.0%', '95.0%', '99.7%', '68.0%', '95.0%', '99.7%']\n",
    "colors = [8, 1, 3, 5, 1, 3, 5]\n",
    "\n",
    "p = visual.multline_plot(x_data, y_data,\n",
    "                         legend_label=legends, \n",
    "                         title='Light Curve Derivative',\n",
    "                         color_index=colors,\n",
    "                         y_axis={'label': 'Intensity'},\n",
    "                         x_axis={'label': 'Time index'})\n",
    "visual.show_plot(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Threshold analysis](./images/nb_gallery/bokeh_plot(7).png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now one can create a `listener` to check each peak and create the respective eclipse label. First it is necessary to check two states: the first is the state of `in_eclipse` and the second the `out_eclipse`. Which will map when the series goes into one eclipse, then out of the eclipse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T21:39:10.341868Z",
     "start_time": "2020-02-20T21:39:10.327878Z"
    }
   },
   "outputs": [],
   "source": [
    "trigger, out_eclipse = False, True # because it starts out of the eclipse\n",
    "light_variation = np.diff(sm1) / time_sample\n",
    "light_variation = light_variation.tolist()\n",
    "threshold = [-1*variation_stan, 1*variation_stan]\n",
    "\n",
    "light_state = [False]\n",
    "size = len(light_variation)\n",
    "for k in range(size):\n",
    "    if out_eclipse:\n",
    "        if light_variation[k] < threshold[0]:\n",
    "            out_eclipse = False\n",
    "    else:\n",
    "        if (light_variation[k-1] > threshold[1]) \\\n",
    "            and (light_variation[k] < threshold[1]):\n",
    "            out_eclipse = True\n",
    "    light_state.append(out_eclipse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With those results, lets see if we can plot the light curve and highlight the moments where we have a suposed eclipse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T21:39:12.710003Z",
     "start_time": "2020-02-20T21:39:12.633205Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "in_eclipse = np.ma.masked_where(np.array(light_state), sm1)\n",
    "out_eclipse = np.ma.masked_where(~np.array(light_state), sm1)\n",
    "\n",
    "x_data = data['t'][index]\n",
    "y_data = [in_eclipse, out_eclipse]\n",
    "legends= ['In eclipse', 'Out eclipse']\n",
    "colors = [3, 7]\n",
    "\n",
    "p1 = visual.multline_plot(x_data, y_data,\n",
    "                         legend_label=legends,\n",
    "                         title='Light Curve Derivative',\n",
    "                         color_index=colors,\n",
    "                         y_axis={'label': 'Intensity'},\n",
    "                         x_axis={'label': 'Time index'})\n",
    "visual.show_plot(p1, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Eclipse labeled](./images/nb_gallery/bokeh_plot(8).png)\n",
    "![Threshold reference](./images/nb_gallery/bokeh_plot(9).png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint!! One thing that we also need to do, is change the `index` variable and check each exoplanet curve and see if we could ensure that this algorithm works for most of then. And also with the bright stars and red giants...\n",
    "\n",
    "After that we are ready to engage on more complex analysis, such as statistical approaches and machine learning techniques!\n",
    "\n",
    "> _**Notice one interesting thing: using the derivative approach, the steady state information of the light curve is automatically discarded! This is a hell of deal, since it is already a filter that mitigates the influence of low frequency siganls and highlight the effect of high frequency ones!!!! O.o Crazy!! Since we only want to analyse the bahavior of the high descent variation, when there are eclipses, the signal derivation is the approach that most highlight this feature. :)**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation algorithms\n",
    "\n",
    "---\n",
    "\n",
    "Here, the algorithm presented above is implemented for each time serie curve and then we generate a file with the pre-processed data. So for that we need to run the following the next procedure. Since all time series are already resampled, only the procedure of filtering and labelling are necessary to be made for all time series. This is the function, that provided the resampled time series, it returns a the filtered and labeled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_series(data=None, window=33, algorithm=\"hamming\"):\n",
    "    '''\n",
    "        This is the function used to filter all time series readed\n",
    "        in a batch process. And it could take some time to run...\n",
    "    '''\n",
    "    filtered_curves = {'r':[],'y':[],'t':[],'i':[], 'lab':[]}\n",
    "    for curve, time, init in zip(data['y'], data['t'], data['ti']):\n",
    "        filt_curve = pyasl.smooth(curve, window, algorithm)\n",
    "        filtered_curves['r'].append( curve )\n",
    "        filtered_curves['y'].append( filt_curve )\n",
    "        filtered_curves['t'].append( time )\n",
    "        filtered_curves['i'].append( init )\n",
    "    filtered_curves['lab'] = data['lab']\n",
    "    return filtered_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filtered_data = filter_series(data=data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the filtered data, we could use the derivative algorithm to label each time series..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_series(data=None, time_sample=None, std_num=3):\n",
    "    '''\n",
    "        This is the function used to label the eclipses part of \n",
    "        the time series, using the filtered lgiht curve data in \n",
    "        a batch process. And it could take some time to run...\n",
    "    '''\n",
    "    data['eclipse_labels'] = []\n",
    "    for curve in data['y']:\n",
    "        derivative = np.diff( curve ) / time_sample\n",
    "        mean, stan = np.mean( derivative ), np.std( derivative )\n",
    "        derivative, threshold = derivative - mean, std_num * stan \n",
    "        light_state, out_eclipse = [False], True\n",
    "        for k in range(len(derivative)):\n",
    "            if out_eclipse:\n",
    "                if derivative[k] < - threshold:\n",
    "                    out_eclipse = False\n",
    "            else:\n",
    "                if (derivative[k-1] > threshold) \\\n",
    "                    and (derivative[k] < threshold):\n",
    "                    out_eclipse = True\n",
    "            light_state.append( out_eclipse )\n",
    "        data['eclipse_labels'].append( light_state )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filtered_data = label_series(data=filtered_data, time_sample=time_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Know we have some structure data in the `filtered_data` variable that is actually pretty suitable for Python users, which is composed only by `dict`, `list`, `array` and `datetime`. And follows this particular structure:\n",
    "\n",
    "```Json\n",
    "{\n",
    "    'r': [\n",
    "        array(...),\n",
    "        array(...),\n",
    "          ⋮\n",
    "        array(...)\n",
    "    ],\n",
    "    'y': [\n",
    "        array(...),\n",
    "        array(...),\n",
    "          ⋮\n",
    "        array(...)\n",
    "    ],\n",
    "    't': [\n",
    "        array(...),\n",
    "        array(...),\n",
    "          ⋮\n",
    "        array(...)\n",
    "    ],\n",
    "    'i': [\n",
    "        datetime,\n",
    "        datetime,\n",
    "          ⋮\n",
    "        datetime\n",
    "    ],\n",
    "    'lab': [\n",
    "        str,\n",
    "        str,\n",
    "         ⋮\n",
    "        str\n",
    "    ],\n",
    "}\n",
    "```\n",
    "\n",
    "where each key is:\n",
    "\n",
    "- `r` : The raw intensity of each curve\n",
    "- `y` : The filtered intensity of each curve\n",
    "- `t` : The time index initialized in 0 of each curve\n",
    "- `i` : The time of the first sample of each curve\n",
    "- `lab`: The string with the label of this curve\n",
    "\n",
    "As an example, to fetch the filtered intesity samples of the third curve, one just:\n",
    "\n",
    "```Python\n",
    "data = filtered_data['y'][2]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save pre-processed data\n",
    "\n",
    "---\n",
    "\n",
    "## Save as _.mat_ file\n",
    "\n",
    "This is just a function that will save the filtered data as a MATLAB data file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "\n",
    "file_path = './filtered.mat'\n",
    "sio.savemat(file_name=file_path, mdict=filtered_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save as _.pickle_ file\n",
    "\n",
    "This is a fucntion to save the data as a _pickle_ file to save the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_path = './filtered.pkl'\n",
    "\n",
    "output = open(file_path, 'wb')\n",
    "pickle.dump(filtered_data, output)\n",
    "output.close()\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "mimetype": "text/x-python",
  "name": "python",
  "notify_time": "5",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "871.25px",
    "left": "1394px",
    "top": "109.25px",
    "width": "214.75px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "version": 3,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
