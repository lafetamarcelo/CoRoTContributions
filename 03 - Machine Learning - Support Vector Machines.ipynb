{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision trees\n",
    "\n",
    "The approach presented in this path, will be one that uses the classical Decision Trees to manage the classification problem. Most of the steps presented here will repeat itself from the XGBoost path, but the algorithm analysis will be a little bit different. This shows that by following the procedure proposed in this study, in general, leads to an interesting pipeline that enables the user to run several machine learning algorithms from the same rundown. Also, this algorithm will be used for each generated feature, namelly:\n",
    "\n",
    "- Periodograms\n",
    "- Bayes Similarity\n",
    "- Hidden Markov Models\n",
    "\n",
    "All approaches will pass trough the common machine learning pipeline, where we must:\n",
    "\n",
    "- Normalize the data (if necessary)\n",
    "- Divide the data between trainning and testing\n",
    "- Search the hyper parameters\n",
    "- Cross validate the models\n",
    "- Analyse the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Periodograms\n",
    "\n",
    "The application using the periodograms is actually pretty simple, now that the data is prepared and all of those preprocessing from last pipeline step is already done. The algorithm became straigh forward. First it is necessary to read the features generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "file_name = './features/freq_data/freq_data.pkl' \n",
    "with open(file_name, 'rb') as file:\n",
    "    freq_data = pickle.load(file)\n",
    "freq_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulate features\n",
    "\n",
    "After reading the data, it is necessary to create the classical regression structure model in the format $Y = f\\left(\\Theta, X\\right)$, normalize the feature data and encode any possible label data into numerical classes. This is just the preparation for the machine leaning algorithm to guarantee that the provided info is properlly designed for any machine learning classical form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Create the label encoder\n",
    "le_freq = preprocessing.LabelEncoder()\n",
    "le_freq.fit(freq_data['labels'])\n",
    "\n",
    "# Define the regression model\n",
    "regressors = preprocessing.normalize(freq_data['features']['spec'])\n",
    "outputs = le_freq.transform(freq_data['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also it is interesting to reduce the features dimension to build a simpler model. It is not necessary to create a classifier with such amount (12758 features...) of features. There are several techniques that can be used to reduce the features dimensions. The Principal Component Analisys, is very effective when dealing with high dimensional data. Here the `PCA` algorithm from the `sklearn` library is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create the PCA decomposer\n",
    "pca_dec = PCA(n_components=70)#, svd_solver='arpack')\n",
    "\n",
    "# Train the PCA object\n",
    "pca_dec.fit(regressors)\n",
    "\n",
    "# Transform the data using\n",
    "# the PCA model\n",
    "pca_regressor = pca_dec.transform(regressors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test data split\n",
    "\n",
    "Next it is necessary to segregate the data into a set for validation and one for trainning the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( \n",
    "    pca_regressor, outputs, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper tunning\n",
    "\n",
    "We could consider tunning the model hyper parameters to answer questions such as:\n",
    "\n",
    "- Wich value of `n_estimators` is the best for this model and data?\n",
    "- Wich cost function is the best to be selected as `objective` for this model?\n",
    "\n",
    "We could do a hyper search, to find the best hyper parameters for this model, automating the hyper parameter selection. There are several already builded algorithms to optimize this parameter search, and build find with high performance the best parameters, provided a set of possible values. But, to understand what those algorithms actually does, we could once build our own search algorithm...\n",
    "\n",
    "As an example, lets run a first handly defined hyper parameter tunning using the confusion matrix of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Define the model parameters \n",
    "param_dist = {\n",
    "    'C' : 0\n",
    "}\n",
    "\n",
    "# Create the range parameters to \n",
    "# search\n",
    "C = np.linspace()\n",
    "\n",
    "# Create the plotting variable\n",
    "plot_vals = {\n",
    "    'true': {\n",
    "        'confirmed targets': [],\n",
    "        'eclipsing binaries': [],\n",
    "    },\n",
    "    'false': {\n",
    "        'confirmed targets': [],\n",
    "        'eclipsing binaries': [],\n",
    "    }\n",
    "}\n",
    "\n",
    "# Estimate and validate each candidate\n",
    "for opt in max_depth:\n",
    "    # Update the model parameters\n",
    "    param_dist['max_depth'] = opt\n",
    "    # Create the xgBoost classifier\n",
    "    clfs = DecisionTreeClassifier(**param_dist)\n",
    "    # Fit the model to the data\n",
    "    clfs.fit(X_train, y_train)\n",
    "    # Estimate the test output\n",
    "    y_pred = clfs.predict(X_test)\n",
    "    # Compute the confusion matrix\n",
    "    conf_mat = confusion_matrix(\n",
    "        y_test, y_pred,\n",
    "        normalize='true')\n",
    "    # Save the confusion matrix\n",
    "    plot_vals['true']['confirmed targets'].append(conf_mat[0,0])\n",
    "    plot_vals['true']['eclipsing binaries'].append(conf_mat[1,1])\n",
    "    plot_vals['false']['confirmed targets'].append(conf_mat[0,1])\n",
    "    plot_vals['false']['eclipsing binaries'].append(conf_mat[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "# Line plot each confidence matrix parameter\n",
    "x_data = [max_depth, max_depth, max_depth, max_depth]\n",
    "y_data = [plot_vals['true']['confirmed targets'],\n",
    "          plot_vals['true']['eclipsing binaries'],\n",
    "          plot_vals['false']['confirmed targets'],\n",
    "          plot_vals['false']['eclipsing binaries']]\n",
    "legends= ['True - C.T.', 'True - E.B.', 'False - C.T.', 'False - E.B.']\n",
    "colors = [6, 7, 2, 3]\n",
    "\n",
    "p = visual.multline_plot(x_data, y_data,\n",
    "                         legend_label=legends, \n",
    "                         title='Hyper parameter search - Confusion parameters plot',\n",
    "                         color_index=colors,\n",
    "                         y_axis={'label': 'Proportion'},\n",
    "                         x_axis={'label': 'n_estimators'})\n",
    "visual.show_plot(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model\n",
    "\n",
    "After running the hyper parameter search we can create a model with the best defined hyper parameters, or setup parameters, and consolidate the model in to the best version for further performance analysis. The model is saved on a particular variable, such as `freq_clf` to be further used in some vote chain model, if further necessary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_dist = {\n",
    "    'random_state': 0,\n",
    "    'criterion' : 'entropy', # or 'entropy' for information gain\n",
    "    'max_depth' : max_depth[2]\n",
    "}\n",
    "\n",
    "# Create the model classifier\n",
    "freq_clf = DecisionTreeClassifier(**param_dist)\n",
    "\n",
    "# Train the model\n",
    "freq_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "In this part it is presented the results from the classification algorithm. Both regarding the data visualization and the model classification quality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "disp = plot_confusion_matrix(freq_clf, X_test, y_test,\n",
    "                             display_labels=le_freq.classes_,\n",
    "                             cmap=plt.cm.Blues,\n",
    "                             normalize='true')\n",
    "disp.ax_.set_title('Periodogram Classifier - Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "\n",
    "From this results it is possible to see that the classifier using periodogram amplitudes can get some interesting knowledge on the eclipsing binaries classification, but could not extract any information to classify properly the confirmed targets. It is almost as if the model is blind when an exo planet periodogram is presented as feature to the algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes likelihood\n",
    "\n",
    "Here we will read the Naive Bayes model parameters estimated for each light curve and use this information as feature for the xgBoost classifier. To start this approach, we must first read the Bayes features saved from last step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "file_name = './features/bayes_data/nx_12/bayes_data.pkl'\n",
    "with open(file_name, 'rb') as file:\n",
    "    bayes_data = pickle.load(file)\n",
    "bayes_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulate features\n",
    "\n",
    "After reading the data, it is necessary to create the classical regression structure model in the format $Y = f\\left(\\Theta, X\\right)$, normalize the feature data and encode any possible label data into numerical classes. This is just the preparation for the machine leaning algorithm to guarantee that the provided info is properlly designed for any machine learning classical form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Create the label encoder\n",
    "le_bayes = preprocessing.LabelEncoder()\n",
    "le_bayes.fit(bayes_data['labels'])\n",
    "\n",
    "# Define the regression model\n",
    "regressors = preprocessing.normalize(bayes_data['features']['params'])\n",
    "outputs = le_bayes.transform(bayes_data['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test data split\n",
    "\n",
    "Next it is necessary to segregate the data into a set for validation and one for trainning the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( \n",
    "    regressors, outputs, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper tunning\n",
    "\n",
    "We could consider tunning the model hyper parameters to answer questions such as:\n",
    "\n",
    "- Wich value of `n_estimators` is the best for this model and data?\n",
    "- Wich cost function is the best to be selected as `objective` for this model?\n",
    "\n",
    "We could do a hyper search, to find the best hyper parameters for this model, automating the hyper parameter selection. There are several already builded algorithms to optimize this parameter search, and build find with high performance the best parameters, provided a set of possible values. But, to understand what those algorithms actually does, we could once build our own search algorithm...\n",
    "\n",
    "As an example, lets run a first handly defined hyper parameter tunning using the confusion matrix of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Define the model parameters \n",
    "param_dist = {\n",
    "    'random_state' : 0,\n",
    "    'criterion' : 'gini', # or 'entropy' for information gain\n",
    "    'max_depth' : 0\n",
    "}\n",
    "\n",
    "# Create the range parameters to \n",
    "# search\n",
    "max_features_dim = X_train.shape[1]\n",
    "max_depth = [k + 1 for k in range(max_features_dim)]\n",
    "\n",
    "# Create the plotting variable\n",
    "plot_vals = {\n",
    "    'true': {\n",
    "        'confirmed targets': [],\n",
    "        'eclipsing binaries': [],\n",
    "    },\n",
    "    'false': {\n",
    "        'confirmed targets': [],\n",
    "        'eclipsing binaries': [],\n",
    "    }\n",
    "}\n",
    "\n",
    "# Estimate and validate each candidate\n",
    "for opt in max_depth:\n",
    "    # Update the model parameters\n",
    "    param_dist['max_depth'] = opt\n",
    "    # Create the xgBoost classifier\n",
    "    clfs = DecisionTreeClassifier(**param_dist)\n",
    "    # Fit the model to the data\n",
    "    clfs.fit(X_train, y_train)\n",
    "    # Estimate the test output\n",
    "    y_pred = clfs.predict(X_test)\n",
    "    # Compute the confusion matrix\n",
    "    conf_mat = confusion_matrix(\n",
    "        y_test, y_pred,\n",
    "        normalize='true')\n",
    "    # Save the confusion matrix\n",
    "    plot_vals['true']['confirmed targets'].append(conf_mat[0,0])\n",
    "    plot_vals['true']['eclipsing binaries'].append(conf_mat[1,1])\n",
    "    plot_vals['false']['confirmed targets'].append(conf_mat[0,1])\n",
    "    plot_vals['false']['eclipsing binaries'].append(conf_mat[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "# Line plot each confidence matrix parameter\n",
    "x_data = [max_depth, max_depth, max_depth, max_depth]\n",
    "y_data = [plot_vals['true']['confirmed targets'],\n",
    "          plot_vals['true']['eclipsing binaries'],\n",
    "          plot_vals['false']['confirmed targets'],\n",
    "          plot_vals['false']['eclipsing binaries']]\n",
    "legends= ['True - C.T.', 'True - E.B.', 'False - C.T.', 'False - E.B.']\n",
    "colors = [6, 7, 2, 3]\n",
    "\n",
    "p = visual.multline_plot(x_data, y_data,\n",
    "                         legend_label=legends, \n",
    "                         title='Hyper parameter search - Confusion parameters plot',\n",
    "                         color_index=colors,\n",
    "                         y_axis={'label': 'Proportion'},\n",
    "                         x_axis={'label': 'n_estimators'})\n",
    "visual.show_plot(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model\n",
    "\n",
    "After running the hyper parameter search we can create a model with the best defined hyper parameters, or setup parameters, and consolidate the model in to the best version for further performance analysis. The model is saved on a particular variable, such as `bayes_clf` to be further used in some vote chain model, if further necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_dist = {\n",
    "    'random_state': 0,\n",
    "    'criterion' : 'gini', # or 'entropy' for information gain\n",
    "    'max_depth' : max_depth[3]\n",
    "}\n",
    "\n",
    "# Create the model classifier\n",
    "bayes_clf = DecisionTreeClassifier(**param_dist)\n",
    "\n",
    "# Train the model\n",
    "bayes_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "In this part it is presented the results from the classification algorithm. Both regarding the data visualization and the model classification quality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "disp = plot_confusion_matrix(bayes_clf, X_test, y_test,\n",
    "                             display_labels=le_bayes.classes_,\n",
    "                             cmap=plt.cm.Blues,\n",
    "                             normalize='true')\n",
    "disp.ax_.set_title('Bayes Classifier - Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "\n",
    "From this results it is possible to see that the classifier using Naive Bayes estimated model parameters is able to highly characterize the eclipsing binaries, and has acceptable classification performance for the confirmed targets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hidden Markov Models\n",
    "\n",
    "Here we use the model estimated from the Hidden Markov Models library, wich is the estimated $A$ matrix, or the so called transition probability matrices as feature for the learning classifier. For that we must read the pickle file with the desired features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "file_name = './features/hmm_data/nx_20/hmm_data.pkl'\n",
    "with open(file_name, 'rb') as file:\n",
    "    hmm_data = pickle.load(file)\n",
    "hmm_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulate features\n",
    "\n",
    "After reading the data, it is necessary to create the classical regression structure model in the format $Y = f\\left(\\Theta, X\\right)$, normalize the feature data and encode any possible label data into numerical classes. This is just the preparation for the machine leaning algorithm to guarantee that the provided info is properlly designed for any machine learning classical form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Encode the label\n",
    "le_hmm = preprocessing.LabelEncoder()\n",
    "le_hmm.fit( hmm_data['labels'] )\n",
    "\n",
    "# Define the model order\n",
    "feat = hmm_data['features']\n",
    "nx = feat['prob_matrix'][0].shape[0]\n",
    "\n",
    "regressors = []\n",
    "for phi in feat['prob_matrix']:\n",
    "    # Reshape the regressor\n",
    "    reg = phi.reshape(nx*nx)\n",
    "    # Add to the regressors\n",
    "    regressors.append(reg)   \n",
    "# Normalize the regressors\n",
    "regressors = preprocessing.normalize(regressors)\n",
    "# Define outputs as encoded variables\n",
    "outputs = le_hmm.transform(hmm_data['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test data split\n",
    "\n",
    "Next it is necessary to segregate the data into a set for validation and one for trainning the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( \n",
    "    regressors, outputs, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper tunning\n",
    "\n",
    "We could consider tunning the model hyper parameters to answer questions such as:\n",
    "\n",
    "- Wich value of `n_estimators` is the best for this model and data?\n",
    "- Wich cost function is the best to be selected as `objective` for this model?\n",
    "\n",
    "We could do a hyper search, to find the best hyper parameters for this model, automating the hyper parameter selection. There are several already builded algorithms to optimize this parameter search, and build find with high performance the best parameters, provided a set of possible values. But, to understand what those algorithms actually does, we could once build our own search algorithm...\n",
    "\n",
    "As an example, lets run a first handly defined hyper parameter tunning using the confusion matrix of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Define the model parameters \n",
    "param_dist = {\n",
    "    'random_state' : 0,\n",
    "    'criterion' : 'gini', # or 'entropy' for information gain\n",
    "    'max_depth' : 0\n",
    "}\n",
    "\n",
    "# Create the range parameters to \n",
    "# search\n",
    "max_features_dim = X_train.shape[1]\n",
    "max_depth = [k + 1 for k in range(max_features_dim)]\n",
    "\n",
    "# Create the plotting variable\n",
    "plot_vals = {\n",
    "    'true': {\n",
    "        'confirmed targets': [],\n",
    "        'eclipsing binaries': [],\n",
    "    },\n",
    "    'false': {\n",
    "        'confirmed targets': [],\n",
    "        'eclipsing binaries': [],\n",
    "    }\n",
    "}\n",
    "\n",
    "# Estimate and validate each candidate\n",
    "for opt in max_depth:\n",
    "    # Update the model parameters\n",
    "    param_dist['max_depth'] = opt\n",
    "    # Create the xgBoost classifier\n",
    "    clfs = DecisionTreeClassifier(**param_dist)\n",
    "    # Fit the model to the data\n",
    "    clfs.fit(X_train, y_train)\n",
    "    # Estimate the test output\n",
    "    y_pred = clfs.predict(X_test)\n",
    "    # Compute the confusion matrix\n",
    "    conf_mat = confusion_matrix(\n",
    "        y_test, y_pred,\n",
    "        normalize='true')\n",
    "    # Save the confusion matrix\n",
    "    plot_vals['true']['confirmed targets'].append(conf_mat[0,0])\n",
    "    plot_vals['true']['eclipsing binaries'].append(conf_mat[1,1])\n",
    "    plot_vals['false']['confirmed targets'].append(conf_mat[0,1])\n",
    "    plot_vals['false']['eclipsing binaries'].append(conf_mat[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "# Line plot each confidence matrix parameter\n",
    "x_data = [max_depth, max_depth, max_depth, max_depth]\n",
    "y_data = [plot_vals['true']['confirmed targets'],\n",
    "          plot_vals['true']['eclipsing binaries'],\n",
    "          plot_vals['false']['confirmed targets'],\n",
    "          plot_vals['false']['eclipsing binaries']]\n",
    "legends= ['True - C.T.', 'True - E.B.', 'False - C.T.', 'False - E.B.']\n",
    "colors = [6, 7, 2, 3]\n",
    "\n",
    "p = visual.multline_plot(x_data, y_data,\n",
    "                         legend_label=legends, \n",
    "                         title='Hyper parameter search - Confusion parameters plot',\n",
    "                         color_index=colors,\n",
    "                         y_axis={'label': 'Proportion'},\n",
    "                         x_axis={'label': 'n_estimators'})\n",
    "visual.show_plot(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model\n",
    "\n",
    "After running the hyper parameter search we can create a model with the best defined hyper parameters, or setup parameters, and consolidate the model in to the best version for further performance analysis. The model is saved on a particular variable, such as `hmm_clf` to be further used in some vote chain model, if further necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_dist = {\n",
    "    'random_state': 0,\n",
    "    'criterion' : 'gini', # or 'entropy' for information gain\n",
    "    'max_depth' : max_depth[0]\n",
    "}\n",
    "\n",
    "# Create the model classifier\n",
    "hmm_clf = DecisionTreeClassifier(**param_dist)\n",
    "\n",
    "# Train the model\n",
    "hmm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Here we include some visualization results for the xgBoost algorithm classification. As the first result, we just print the model eval metrics, here the *log loss* of the model, for both the trainning and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "disp = plot_confusion_matrix(hmm_clf, X_test, y_test,\n",
    "                             display_labels=le_hmm.classes_,\n",
    "                             cmap=plt.cm.Blues,\n",
    "                             normalize='true')\n",
    "disp.ax_.set_title('Confusion matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
